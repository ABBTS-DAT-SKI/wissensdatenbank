{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Modul DAT-SKI: Data Science und K\u00fcnstliche Intelligenz","text":""},{"location":"#portrat","title":"Portr\u00e4t","text":"<p>Das Modul vermittelt praxisorientiertes Wissen \u00fcber Data Science und K\u00fcnstliche Intelligenz und zeigt, wie diese Technologien in der realen Welt angewendet werden k\u00f6nnen. Ziel ist es, den Studierenden ein tiefgehendes Verst\u00e4ndnis f\u00fcr die Verarbeitung, Analyse und Interpretation von Daten zu vermitteln und deren Bedeutung f\u00fcr datengetriebene Entscheidungen zu verdeutlichen.</p> <p>Data Science ist in der heutigen Zeit unerl\u00e4sslich, da es Unternehmen und Organisationen erm\u00f6glicht, wertvolle Erkenntnisse aus riesigen Datenmengen zu gewinnen und fundierte Entscheidungen zu treffen. Ein praktisches Beispiel zeigt dies im Bereich der Geb\u00e4udeautomation: Sensoren, die Temperatur, Luftfeuchtigkeit oder CO2-Konzentrationen messen, liefern kontinuierlich Daten, die zur Optimierung der Raumluftqualit\u00e4t und Energieeffizienz eingesetzt werden k\u00f6nnen. Durch Data Science k\u00f6nnen diese Sensordaten in Echtzeit analysiert werden, um beispielsweise automatisch die Heizung oder Bel\u00fcftung zu steuern, wodurch nicht nur Kosten gesenkt, sondern auch der Komfort und die Gesundheit der Nutzer verbessert werden.</p> <p>Anstelle von reinem Frontalunterricht wird ein \"Learning by Doing\"-Ansatz verfolgt: Die Studierenden arbeiten aktiv mit echten Daten, l\u00f6sen praxisorientierte Aufgaben und wenden das erlernte Wissen direkt an. Dabei sorgt die Mini-Challenge daf\u00fcr, dass nicht nur theoretische Kenntnisse, sondern auch praktische F\u00e4higkeiten aufgebaut werden.</p>"},{"location":"#lernergebnisse","title":"Lernergebnisse","text":""},{"location":"#le1-grundlagen-der-datenanalyse","title":"LE1: Grundlagen der Datenanalyse","text":"<p>Daten enthalten oft Unsauberkeiten, die eine genaue Analyse erschweren. Sie kennen verschiedene Techniken der Datenbereinigung, einschliesslich der Behandlung fehlender Werte, Duplikate und Ausreisser. Sie k\u00f6nnen Daten aus verschiedenen Quellen homogenisieren und explorative Datenanalysen durchf\u00fchren, um Muster, Trends und Anomalien zu identifizieren. Sie wenden Resampling-Methoden an, um zeitabh\u00e4ngige Daten sinnvoll zu interpolieren und zu gl\u00e4tten.</p>"},{"location":"#le2-einfuhrung-in-datenbanken","title":"LE2: Einf\u00fchrung in Datenbanken","text":"<p>Datenbanken erm\u00f6glichen die strukturierte Speicherung und Abfrage grosser Datenmengen. Sie beherrschen die grundlegenden SQL-Abfragen, insbesondere SELECT-Statements, und k\u00f6nnen relationale und nicht-relationale Datenbanken unterscheiden. Sie wissen, wann der Einsatz von Time-Series-Datenbanken sinnvoll ist und verstehen die jeweiligen Vor- und Nachteile.</p>"},{"location":"#le3-maschinelles-lernen-grundkonzepte","title":"LE3: Maschinelles Lernen Grundkonzepte","text":"<p>Maschinelles Lernen erm\u00f6glicht datengetriebene Entscheidungen. Sie wenden Scikit-learn an, um einfache Regressions- und Klassifikationsmodelle zu implementieren. Sie f\u00fchren Trainings- und Testprozesse korrekt durch und verstehen den Zweck von Train-Test-Splits. Zudem sind Sie in der Lage, die Aussagekraft eines Modells durch Cross-Validation zu bewerten.</p>"},{"location":"#le4-datenvisualisierung","title":"LE4: Datenvisualisierung","text":"<p>Die visuelle Darstellung von Daten ist entscheidend f\u00fcr deren Interpretation. Sie k\u00f6nnen mit Seaborn aussagekr\u00e4ftige Diagramme erstellen und verstehen deren Bedeutung f\u00fcr die Datenanalyse. Zudem sind Sie in der Lage, ein interaktives Dashboard mit Grafana aufzusetzen und dessen Anwendungsf\u00e4lle kritisch zu beurteilen.</p>"},{"location":"#le5-ethik-und-datenschutz","title":"LE5: Ethik und Datenschutz","text":"<p>Der verantwortungsvolle Umgang mit Daten ist essenziell. Sie verstehen die Grundlagen des Datenschutzes und k\u00f6nnen Datenschutzpraktiken anwenden. Sie kennen den Zweck und Aufbau einer Datenschutzerkl\u00e4rung und k\u00f6nnen ethische Fragestellungen im Zusammenhang mit Datenwissenschaft erkennen und diskutieren, insbesondere in den Bereichen Geb\u00e4udeautomation und Umwelttechnik.</p>"},{"location":"#aufgaben","title":"Aufgaben","text":"<p>Alle Aufgaben sind ebenfalls auf GitHub verf\u00fcgbar: https://github.com/ABBTS-DAT-SKI/aufgaben</p>"},{"location":"#leistungsnachweise","title":"Leistungsnachweise","text":"<p>Der Kompetenznachweis erfolgt durch:</p> <ul> <li>Eine benotete Abgabe (Mini-Challenge). Weitere Informationen dazu finden Sie hier.</li> <li>Eine schriftliche Pr\u00fcfung in der 10. Semesterwoche. Zur schriftlichen Pr\u00fcfung darf ein Spick mitgebracht werden. Dieser muss physisch sein (auf Papier), darf aber auf dem Computer erstellt und ausgedruckt werden.</li> </ul>"},{"location":"#wissensdatenbank","title":"Wissensdatenbank","text":"<p>Die Wissensdatenbank ist eine zentrale Sammlung aller Lerninhalte, Beispiele und Erkl\u00e4rungen. Sie unterst\u00fctzt Studierende dabei, das Gelernte zu vertiefen und direkt in Aufgaben anzuwenden. Sollte Material fehlen oder unklar sein, wird gebeten, die Dozenten darauf aufmerksam zu machen, damit es zeitnah erg\u00e4nzt werden kann.</p>"},{"location":"mini_challenge/","title":"Mini-Challenge","text":"<p>In dieser Mini-Challenge k\u00f6nnen Sie Ihr Wissen anwenden, um reale Sensordaten zu untersuchen und wichtige Zusammenh\u00e4nge zwischen Variablen zu modellieren.</p> <p>Ziel ist es, einen Datensatz auszuw\u00e4hlen, diesen gr\u00fcndlich zu bereinigen, die Datenanalyse durchzuf\u00fchren und anschliessend mithilfe eines ML-Modells eine Vorhersage oder Beziehung zwischen zwei Variablen zu quantifizieren und zu visualisieren. Der Fokus liegt auf der praktischen Umsetzung des gelernten Stoffes und der kritischen Auseinandersetzung mit den Daten.</p>"},{"location":"mini_challenge/#vorgehen","title":"Vorgehen","text":"<p>W\u00e4hlen Sie einen Sensordatensatz aus, der in mehreren Dateien vorliegt. Sie k\u00f6nnen Daten aus einem fr\u00fcheren Modul oder aus \u00f6ffentlich zug\u00e4nglichen Quellen wie Kaggle nutzen. Kl\u00e4ren Sie Ihre Auswahl mit den Dozenten ab, um sicherzustellen, dass der Datensatz geeignet ist.</p> <p>Hier einige Beispielsdatens\u00e4tze:</p> <ul> <li>Smart Building System</li> <li>CU-BEMS, smart building energy and IAQ data</li> <li>ASHRAE Global Occupant Behavior Database</li> </ul> <p>Bereinigen Sie die Daten gr\u00fcndlich, indem Sie unvollst\u00e4ndige, fehlerhafte oder inkonsistente Daten behandeln. Vor der Analyse m\u00fcssen alle Dateien in eine einzige Datei zusammengef\u00fchrt werden, um eine einheitliche und umfassende Datengrundlage zu erhalten.</p> <p>Untersuchen Sie den Datensatz explorativ und beschreiben Sie auff\u00e4llige Muster, Anomalien oder fehlende Werte. Erstellen Sie Visualisierungen, die Ihnen helfen, die Daten besser zu verstehen.</p> <p>Identifizieren Sie zwei Variablen, deren Zusammenspiel Sie analysieren m\u00f6chten, und w\u00e4hlen Sie ein geeignetes ML-Modell, um eine Korrelation zu beschreiben. Eine lineare Regression eignet sich beispielsweise gut, um Zusammenh\u00e4nge zwischen zwei kontinuierlichen Variablen zu visualisieren. Fassen Sie die Ergebnisse zusammen und interpretieren Sie, was die Modellierung \u00fcber die Beziehung der Variablen aussagt.</p> <p>Warning</p> <p>Dies ist eine Einzelarbeit. Besprechen Sie jedoch die genauen Auftragsdetails mit den Dozenten, um sicherzustellen, dass Ihre Herangehensweise sinnvoll ist. W\u00e4hlen Sie eine Fragestellung, die eine ausgewogene Balance zwischen Herausforderung und Machbarkeit bietet.</p> <p>Tip</p> <p>Die Nutzung von ChatGPT oder anderen KI-Tools ist erlaubt und f\u00fcr Wissensfragen sogar deutlich empfohlen. Jedoch sollte die Nutzung in einem Mass stattfinden, dass Sie verstehen, was gemacht wird und warum gewisse Entscheidungen getroffen werden. In der Abschlusspr\u00fcfung k\u00f6nnen Fragen zur Mini-Challenge gestellt werden, die dies testen.</p>"},{"location":"mini_challenge/#abgabe","title":"Abgabe","text":"<p>Die Ergebnisse dokumentieren Sie in einem Bericht (als PDF), der maximal 8 Seiten umfasst (ohne Titelblatt, Inhaltsverzeichnis oder \u00c4hnliches). Der Bericht sollte eine klare Struktur haben und die folgenden Punkte abdecken:  </p> <ul> <li>Beschreibung des Datensatzes und der Zielsetzung  </li> <li>Vorgehen bei der Datenanalyse und -bereinigung  </li> <li>Beschreibung der Modellierung und Interpretation der Ergebnisse  </li> </ul> <p>Geben Sie des Weiteren den Code als ZIP oder Notebook ab.</p> <p>Die Abgabe des Berichts und Code erfolgt bis sp\u00e4testens 01.06.2025.</p>"},{"location":"mini_challenge/#check-in","title":"Check-In","text":"<p>Um sich zu stellen, dass Sie auf dem richtigen Weg sind ist es wichtig, dass Sie bis Ende Semesterwoche 5 auf Teams das Check-In dieser Mini-Challenge abgeben. Dort sollen Sie in zwei bis drei S\u00e4tzen kurz beschreiben welchen Datensatz Sie verwenden und welches Analyseziel das Sie verfolgen. Damit k\u00f6nnen wir sicherstellen, dass Sie sich nicht zu viel Aufwand machen und sich der Datensatz auch eignet f\u00fcr diese Mini-Challenge.</p> <p>Wenn Sie bereits fr\u00fcher beginnen wollen, k\u00f6nnen Sie selbstverst\u00e4ndlich uns auch eine E-Mail schreiben. </p>"},{"location":"mini_challenge/#bewertungsraster","title":"Bewertungsraster","text":""},{"location":"mini_challenge/#1-datenauswahl-und-zieldefinition-10","title":"1. Datenauswahl und Zieldefinition (10%)","text":"Punktebereich Beschreibung 5\u201310 Punkte Der Datensatz ist gut gew\u00e4hlt und passt hervorragend zur Zielsetzung der Aufgabe. Das Ziel der Analyse ist klar formuliert. 3\u20134 Punkte Der Datensatz ist gr\u00f6sstenteils geeignet, aber es gibt kleinere Unsicherheiten bei der Zieldefinition oder der Auswahl des Datensatzes. 1\u20132 Punkte Der Datensatz passt nicht gut zur Aufgabe oder das Ziel ist unklar formuliert. 0 Punkte Kein geeigneter Datensatz ausgew\u00e4hlt oder das Ziel ist nicht definiert."},{"location":"mini_challenge/#2-datenbereinigung-30","title":"2. Datenbereinigung (30%)","text":"Punktebereich Beschreibung 20\u201330 Punkte Die Daten wurden zusammengef\u00fchrt und gr\u00fcndlich/systematisch bereinigt. Alle relevanten Probleme (fehlende Werte, Inkonsistenzen, Ausreisser) wurden angemessen adressiert. 10\u201319 Punkte Die Datenbereinigung wurde gr\u00f6sstenteils durchgef\u00fchrt, jedoch gibt es kleinere M\u00e4ngel. 5\u20139 Punkte Die Datenbereinigung ist oberfl\u00e4chlich und viele Probleme wurden nicht adressiert. 0 Punkte Es wurde keine oder eine unzureichende Datenbereinigung durchgef\u00fchrt."},{"location":"mini_challenge/#3-datenanalyse-und-visualisierung-20","title":"3. Datenanalyse und Visualisierung (20%)","text":"Punktebereich Beschreibung 15\u201320 Punkte Die explorative Datenanalyse ist fundiert und umfassend. Alle wichtigen Aspekte des Datensatzes wurden untersucht und aussagekr\u00e4ftige Visualisierungen erstellt. 10\u201314 Punkte Die Datenanalyse ist gr\u00f6sstenteils fundiert, aber einige Aspekte oder Muster wurden m\u00f6glicherweise \u00fcbersehen. 5\u20139 Punkte Die Analyse ist oberfl\u00e4chlich, und die Visualisierungen sind nicht sehr aussagekr\u00e4ftig. 0\u20134 Punkte Es wurden keine aussagekr\u00e4ftigen Visualisierungen oder Analysen durchgef\u00fchrt."},{"location":"mini_challenge/#4-modellierung-und-interpretation-20","title":"4. Modellierung und Interpretation (20%)","text":"Punktebereich Beschreibung 15\u201320 Punkte Ein geeignetes Modell wurde ausgew\u00e4hlt, und die Ergebnisse sind klar interpretiert. Die Wahl des Modells ist gut begr\u00fcndet, und die Interpretation ist nachvollziehbar. 10\u201314 Punkte Ein angemessenes Modell wurde verwendet, aber die Interpretation der Ergebnisse ist unklar oder nicht vollst\u00e4ndig. 5\u20139 Punkte Das Modell ist unpassend oder es fehlt eine klare Begr\u00fcndung. Die Ergebnisse sind schwer zu interpretieren. 0\u20134 Punkte Es wurde kein Modell angewendet oder die Interpretation der Ergebnisse ist unverst\u00e4ndlich."},{"location":"mini_challenge/#5-bericht-20","title":"5. Bericht (20%)","text":"Punktebereich Beschreibung 15\u201320 Punkte Der Bericht ist klar strukturiert, \u00fcbersichtlich und gut dokumentiert. Alle Schritte der Analyse sind nachvollziehbar. Die Dokumentation ist verst\u00e4ndlich und pr\u00e4zise. 10\u201314 Punkte Der Bericht ist gr\u00f6sstenteils strukturiert, aber es fehlen teilweise Details oder er k\u00f6nnte klarer formuliert werden. 5\u20139 Punkte Der Bericht hat strukturelle M\u00e4ngel und wichtige Aspekte sind unzureichend dokumentiert. 0\u20134 Punkte Der Bericht ist unstrukturiert oder unvollst\u00e4ndig. Wichtige Informationen fehlen."},{"location":"python_installation/","title":"Python Installation","text":"<p>Python ist eine der beliebtesten Programmiersprachen der Welt. Sie wird in vielen Bereichen eingesetzt, darunter Webentwicklung, Datenanalyse, k\u00fcnstliche Intelligenz und vieles mehr. Python ist bekannt f\u00fcr seine einfache Syntax und seine Vielseitigkeit, was es zu einer grossartigen Sprache f\u00fcr Anf\u00e4nger und Fortgeschrittene macht.</p> <p>In diesem Guide zeigen wir dir, wie du Python auf deinem Computer installierst und einrichtest, um mit dem Programmieren zu beginnen.</p>"},{"location":"python_installation/#installation-auf-windows","title":"Installation auf Windows","text":"<p>Lade die Python Version 3.12 vom Microsoft Store herunter:</p> <ul> <li>Python 3.12</li> </ul>"},{"location":"python_installation/#packages","title":"Packages","text":"<p>Note</p> <p>Ein Package in Python ist eine Sammlung von Modulen, die bestimmte Funktionen und Werkzeuge enthalten. Ein Modul ist einfach eine Datei, die Python-Code enth\u00e4lt, und ein Package organisiert mehrere dieser Module in einer Struktur, die es einfacher macht, wiederverwendbare Codebl\u00f6cke zu verwalten.</p> <p>Um neue Packages zu installieren, nutze pip. Pip ist ein Paketmanager f\u00fcr Python, der es dir erm\u00f6glicht, Packages aus dem Python Package Index (PyPI) zu installieren und zu verwalten.</p> <p>Um ein Package zu installieren, \u00f6ffne das Terminal (auf Windows heisst es \"Eingabeaufforderung\") und f\u00fchre folgenden Befehl aus: <pre><code>pip install &lt;PACKAGE1&gt; &lt;PACKAGE2&gt; &lt;PACKAGE3&gt; ...\n</code></pre></p> <p>Beispiel: Um das pandas-Package zu installieren, f\u00fchre folgende Befehle aus: <pre><code>pip install pandas\n</code></pre></p> <p>Es k\u00f6nnte sein, dass bei neueren Python-Versionen diese Befehle zu einem Fehler f\u00fchren. Probiere in diesem Fall dies aus: <pre><code>pip install pandas --break-system-packages\n</code></pre></p>"},{"location":"data-engineering/column_manipulations/","title":"Spaltenmanipulation","text":"<p>Die Arbeit mit Daten in Pandas dreht sich oft um das Manipulieren von Spalten in DataFrames. Dieser Leitfaden zeigt dir die wichtigsten Techniken, um Spalten auszuw\u00e4hlen, zu erstellen, zu ver\u00e4ndern und zu analysieren. Mit diesen Grundlagen wirst du in der Lage sein, Daten effektiv zu transformieren und aussagekr\u00e4ftige Informationen zu gewinnen.</p>"},{"location":"data-engineering/column_manipulations/#schnellubersicht-pandas-cheat-sheet","title":"Schnell\u00fcbersicht: Pandas \"Cheat Sheet\"","text":"Operation Syntax Beschreibung Spalte ausw\u00e4hlen <code>df['Spalte']</code> oder <code>df.Spalte</code> Einzelne Spalte als Series Mehrere Spalten <code>df[['Spalte1', 'Spalte2']]</code> Mehrere Spalten als DataFrame Neue Spalte erstellen <code>df['Neue_Spalte'] = Wert</code> F\u00fcgt eine neue Spalte hinzu Bedingte Spalte <code>df.loc[df['Spalte'] &gt; Wert, 'Neue_Spalte'] = Wert</code> F\u00fcgt Werte basierend auf Bedingung ein Spalte umbenennen <code>df.rename(columns={'Alt': 'Neu'})</code> Benennt Spalten um Spalte l\u00f6schen <code>df.drop('Spalte', axis=1)</code> Entfernt eine Spalte Datentyp umwandeln <code>df['Spalte'].astype(Typ)</code> Konvertiert Spalte in anderen Typ Bedingte Transformation (where) <code>df['Spalte'].where(Bedingung, Ersatz)</code> Ersetzt Werte wo Bedingung FALSCH ist Bedingte Transformation (mask) <code>df['Spalte'].mask(Bedingung, Ersatz)</code> Ersetzt Werte wo Bedingung WAHR ist Rundung <code>df['Spalte'].round(Dezimalstellen)</code> Rundet Werte auf Dezimalstellen Zeilen filtern (einfach) <code>df[df['Spalte'] &gt; Wert]</code> Filtert Zeilen nach Bedingung Zeilen filtern (mehrfach) <code>df[(Bedingung1) &amp; (Bedingung2)]</code> Filtert mit UND-Verkn\u00fcpfung Zeilen filtern (Liste) <code>df[df['Spalte'].isin([Wert1, Wert2])]</code> Filtert Zeilen nach Werteliste Statistische Analyse <code>df['Spalte'].describe()</code> Berechnet statistische Kennzahlen Gruppierung <code>df.groupby('Gruppe')['Spalte'].agg()</code> Gruppiert und aggregiert Daten Mehrere Aggregationen <code>df.groupby('Gruppe').agg({'Sp1': 'mean', 'Sp2': 'max'})</code> Verschiedene Aggregationen pro Spalte Position ausw\u00e4hlen (iloc) <code>df.iloc[Zeile, Spalte]</code> Zugriff basierend auf Position Zeilenbereich <code>df.iloc[Start:Ende]</code> W\u00e4hlt Zeilenbereich aus Zellen-Bereich <code>df.iloc[ZeilenStart:Ende, SpaltenStart:Ende]</code> W\u00e4hlt Rechteck aus Zellen aus"},{"location":"data-engineering/column_manipulations/#1-spalten-auswahlen-und-anzeigen","title":"1. Spalten ausw\u00e4hlen und anzeigen","text":"<p>Der erste Schritt bei der Datenanalyse ist oft, bestimmte Spalten auszuw\u00e4hlen. Pandas bietet verschiedene M\u00f6glichkeiten, auf Spalten zuzugreifen - von einzelnen Spalten bis hin zu komplexen Teilmengen. Dies ist grundlegend f\u00fcr alle weiteren Operationen, da du zun\u00e4chst ausw\u00e4hlen musst, mit welchen Daten du arbeiten m\u00f6chtest.</p>"},{"location":"data-engineering/column_manipulations/#einzelne-spalten-auswahlen","title":"Einzelne Spalten ausw\u00e4hlen","text":"<p>In Pandas k\u00f6nnen Spalten auf zwei Arten ausgew\u00e4hlt werden. Die erste Methode mit eckigen Klammern funktioniert immer, w\u00e4hrend die Punktnotation nur bei einfachen Spaltennamen ohne Sonderzeichen oder Leerzeichen funktioniert.</p> <pre><code># Methode 1: Mit eckigen Klammern - die universellere Methode\ntemperatur = sensordaten['Temperatur']\nprint(f\"Typ: {type(temperatur)}\")  # &lt;class 'pandas.core.series.Series'&gt;\n\n# Methode 2: Mit Punktnotation - k\u00fcrzer, aber nur bei einfachen Spaltennamen\ntemperatur = sensordaten.Temperatur  # Gleiches Ergebnis\n</code></pre> <p>Eine einzelne Spalte wird als Pandas Series zur\u00fcckgegeben - ein eindimensionales Array mit Index.</p>"},{"location":"data-engineering/column_manipulations/#mehrere-spalten-auswahlen","title":"Mehrere Spalten ausw\u00e4hlen","text":"<p>Wenn du mit mehreren Spalten gleichzeitig arbeiten m\u00f6chtest, ben\u00f6tigst du doppelte eckige Klammern. Dies ist einer der h\u00e4ufigsten Fehler bei Pandas-Anf\u00e4ngern - vergiss die doppelten Klammern nicht!</p> <pre><code># WICHTIG: Doppelte eckige Klammern f\u00fcr mehrere Spalten!\ntemp_feuchte = sensordaten[['Temperatur', 'Luftfeuchtigkeit']]\nprint(f\"Typ: {type(temp_feuchte)}\")  # &lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Tipp f\u00fcr Anf\u00e4nger: Der Unterschied zwischen <code>df['Spalte']</code> (Series) und <code>df[['Spalte']]</code> (DataFrame) ist wichtig und f\u00fchrt oft zu Verwirrung. Mit doppelten Klammern erh\u00e4ltst du immer einen DataFrame, auch wenn du nur eine Spalte ausw\u00e4hlst.</p>"},{"location":"data-engineering/column_manipulations/#spalten-nach-muster-auswahlen","title":"Spalten nach Muster ausw\u00e4hlen","text":"<p>Oft m\u00f6chtest du Spalten ausw\u00e4hlen, die einem bestimmten Benennungsmuster folgen. Zum Beispiel k\u00f6nntest du alle Temperatur-Sensoren oder alle Aussenmessungen ausw\u00e4hlen wollen. Pandas bietet mehrere Methoden, um Spalten basierend auf ihren Namen zu filtern.</p> <pre><code># Methode 1: Mit filter() und like-Parameter - w\u00e4hlt Spalten aus, die den String enthalten\ntemperatur_spalten = sensordaten.filter(like='Temperatur', axis=1)\n# Ergebnis: DataFrame mit \"Temperatur_Innen\", \"Temperatur_Aussen\", etc.\n\n# Methode 2: Mit startswith() und List Comprehension - pr\u00e4ziser f\u00fcr Pr\u00e4fixe\ntemperatur_spalten = sensordaten[[col for col in sensordaten.columns if col.startswith('Temperatur')]]\n# Gleiches Ergebnis, aber explizit nur Spalten, die mit \"Temperatur\" BEGINNEN\n</code></pre> <p>Bei der zweiten Methode verwendest du die <code>startswith()</code>-Methode, die pr\u00fcft, ob ein String mit einem bestimmten Pr\u00e4fix beginnt. Das ist pr\u00e4ziser als <code>filter(like='...')</code>, das nach dem Vorkommen des Strings irgendwo im Spaltennamen sucht.</p>"},{"location":"data-engineering/column_manipulations/#2-spalten-erstellen-und-transformieren","title":"2. Spalten erstellen und transformieren","text":"<p>Oft reichen die vorhandenen Spalten in deinem Datensatz nicht aus, um alle Fragen zu beantworten. Das Erstellen neuer Spalten, auch \"Feature Engineering\" genannt, ist eine zentrale F\u00e4higkeit in der Datenanalyse. Hier lernst du, wie du aus bestehenden Daten neue Informationen generieren kannst, um tiefere Einblicke zu gewinnen.</p>"},{"location":"data-engineering/column_manipulations/#neue-spalten-hinzufugen","title":"Neue Spalten hinzuf\u00fcgen","text":"<p>Das Erstellen neuer Spalten ist in Pandas besonders einfach - du weist einfach Werte zu einem neuen Spaltennamen zu. Dies k\u00f6nnen konstante Werte, berechnete Werte oder bedingte Werte sein.</p> <pre><code># Neue Spalte mit konstantem Wert - n\u00fctzlich f\u00fcr Kategorisierung oder Kennzeichnung\nsensordaten['Messstation'] = 'Station-1'\n\n# Neue Spalte durch Berechnung - hier am Beispiel einer Einheitenumrechnung\nsensordaten['Temperatur_F'] = sensordaten['Temperatur'] * 9/5 + 32  # Umrechnung in Fahrenheit\n\n# Bedingte Spalte - kategorisiert Werte basierend auf Schwellenwerten\nsensordaten['Temp_Kategorie'] = 'Normal'  # Standardwert f\u00fcr alle Zeilen\nsensordaten.loc[sensordaten['Temperatur'] &gt; 25, 'Temp_Kategorie'] = 'Warm'  # \u00dcberschreibt bei hohen Werten\nsensordaten.loc[sensordaten['Temperatur'] &lt; 20, 'Temp_Kategorie'] = 'K\u00fchl'  # \u00dcberschreibt bei niedrigen Werten\n</code></pre> <p>Die <code>.loc</code>-Methode ist besonders m\u00e4chtig, da sie nur die Zeilen \u00e4ndert, die der Bedingung entsprechen.</p>"},{"location":"data-engineering/column_manipulations/#spalten-umbenennen","title":"Spalten umbenennen","text":"<pre><code># Eine einzelne Spalte umbenennen\nsensordaten = sensordaten.rename(columns={'Temperatur': 'Temp'})\n\n# Mehrere Spalten umbenennen\nsensordaten = sensordaten.rename(columns={\n    'Luftfeuchtigkeit': 'Feuchtigkeit',\n    'CO2': 'co2'\n})\n\n# Alle Spaltennamen in Kleinbuchstaben\nsensordaten.columns = sensordaten.columns.str.lower()\n</code></pre>"},{"location":"data-engineering/column_manipulations/#spalten-loschen","title":"Spalten l\u00f6schen","text":"<pre><code># Eine Spalte l\u00f6schen\nsensordaten = sensordaten.drop('temperatur_f', axis=1)  # axis=1 f\u00fcr Spalten!\n\n# Mehrere Spalten l\u00f6schen\nsensordaten = sensordaten.drop(['messstation', 'temp_kategorie'], axis=1)\n</code></pre>"},{"location":"data-engineering/column_manipulations/#3-datentypen-und-transformationen","title":"3. Datentypen und Transformationen","text":"<p>Die richtigen Datentypen sind entscheidend f\u00fcr effiziente Analysen und korrekte Ergebnisse. In diesem Abschnitt lernst du, wie du Datentypen \u00e4ndern und Spalteninhalte durch mathematische oder bedingte Operationen transformieren kannst. Diese Techniken helfen dir, Daten f\u00fcr spezifische Analysen vorzubereiten und Messwerte in andere Einheiten umzurechnen.</p>"},{"location":"data-engineering/column_manipulations/#typen-umwandeln","title":"Typen umwandeln","text":"<p>Datentypen beeinflussen sowohl den Speicherverbrauch als auch die Funktionsweise von Operationen. Die Umwandlung von Datentypen ist eine wichtige Technik, um Daten richtig zu interpretieren und effizient zu speichern.</p> <pre><code># Integer-Umwandlung - n\u00fctzlich f\u00fcr ganzzahlige Werte und Speicheroptimierung\n# Hier erst runden, dann in Integer umwandeln\nsensordaten['co2'] = sensordaten['co2'].round().astype(int)\n\n# Kategorie-Typ - sehr speichereffizient f\u00fcr Spalten mit wiederholten Textwerten\n# z.B. f\u00fcr \"Warm\", \"Normal\", \"K\u00fchl\", die sich oft wiederholen\nsensordaten['temp_kategorie'] = sensordaten['temp_kategorie'].astype('category')\n\n# Datentypen anzeigen - n\u00fctzlich zur \u00dcberpr\u00fcfung\nprint(sensordaten.dtypes)\n</code></pre> <p>Mit der richtigen Typenwahl kannst du nicht nur Speicher sparen, sondern auch spezifische Funktionen nutzen, die nur f\u00fcr bestimmte Datentypen verf\u00fcgbar sind.</p>"},{"location":"data-engineering/column_manipulations/#mathematische-operationen","title":"Mathematische Operationen","text":"<pre><code># Grundlegende Mathematik\nsensordaten['temp_korrigiert'] = sensordaten['temp'] + 0.5\nsensordaten['temp_normalisiert'] = sensordaten['temp'] / sensordaten['temp'].max()\n\n# Rundungsfunktionen\nsensordaten['temp_gerundet'] = sensordaten['temp'].round(1)  # Eine Dezimalstelle\n</code></pre>"},{"location":"data-engineering/column_manipulations/#bedingte-transformation","title":"Bedingte Transformation","text":"<pre><code># where: Beh\u00e4lt Werte bei TRUE, ersetzt bei FALSE\nsensordaten['co2_gefiltert'] = sensordaten['co2'].where(sensordaten['co2'] &lt; 500, 500)\n\n# mask: Ersetzt Werte bei TRUE, beh\u00e4lt bei FALSE (Gegenteil von where)\nsensordaten['temp_begrenzt'] = sensordaten['temp'].mask(sensordaten['temp'] &gt; 25, 25)\n</code></pre>"},{"location":"data-engineering/column_manipulations/#4-statistik-und-aggregation","title":"4. Statistik und Aggregation","text":"<p>Das Berechnen statistischer Kennzahlen ist ein zentraler Bestandteil der Datenanalyse. Pandas bietet leistungsstarke Funktionen, um einzelne Spalten oder gruppierte Daten zu analysieren. Diese Methoden helfen dir, grundlegende Fragen zu beantworten wie \"Was ist der Durchschnittswert?\" oder \"Wie unterscheiden sich die Werte zwischen verschiedenen Gruppen?\".</p>"},{"location":"data-engineering/column_manipulations/#einfache-statistiken","title":"Einfache Statistiken","text":"<p>Pandas bietet zahlreiche Methoden f\u00fcr statistische Berechnungen. Diese helfen dir, schnell einen \u00dcberblick \u00fcber deine Daten zu gewinnen und wichtige Kennzahlen zu bestimmen.</p> <pre><code># Einzelne Statistiken - n\u00fctzlich f\u00fcr gezielte Fragestellungen\nprint(f\"Mittlere Temperatur: {sensordaten['temp'].mean():.1f}\u00b0C\")\nprint(f\"Minimale Temperatur: {sensordaten['temp'].min():.1f}\u00b0C\") \nprint(f\"Maximale Temperatur: {sensordaten['temp'].max():.1f}\u00b0C\")\n\n# Weitere n\u00fctzliche Statistiken\nprint(f\"Median-Temperatur: {sensordaten['temp'].median():.1f}\u00b0C\")  # Robust gegen Ausreisser\nprint(f\"Standardabweichung: {sensordaten['temp'].std():.1f}\u00b0C\")    # Streuung der Werte\n\n# Alle Statistiken auf einmal mit describe()\nstats = sensordaten['temp'].describe()\nprint(stats)\n</code></pre> <p>Die <code>describe()</code>-Methode ist besonders praktisch, da sie einen umfassenden \u00dcberblick mit Mittelwert, Standardabweichung, Minimum, Maximum und Quantilen liefert.</p>"},{"location":"data-engineering/column_manipulations/#gruppierte-statistiken","title":"Gruppierte Statistiken","text":"<pre><code># Angenommen, wir f\u00fcgen eine Kategoriespalte hinzu\nsensordaten['tageszeit'] = pd.cut(\n    sensordaten.index.hour,\n    bins=[0, 12, 24],\n    labels=['Vormittag', 'Nachmittag'],\n    right=False\n)\n\n# Gruppieren und Mittelwert\nnach_tageszeit = sensordaten.groupby('tageszeit')['temp'].mean()\nprint(nach_tageszeit)\n\n# Mehrere Aggregationen gleichzeitig\ntageszeit_stats = sensordaten.groupby('tageszeit').agg({\n    'temp': ['mean', 'min', 'max'],\n    'co2': 'mean'\n})\nprint(tageszeit_stats)\n</code></pre>"},{"location":"data-engineering/column_manipulations/#5-zeilen-auswahlen-row-slicing","title":"5. Zeilen ausw\u00e4hlen (Row-Slicing)","text":"<p>Neben der Spaltenmanipulation ist die Auswahl bestimmter Zeilen eine wichtige Technik in Pandas. Mit Row-Slicing kannst du Teilmengen deiner Daten basierend auf Positionen oder Bedingungen extrahieren. Diese Methoden sind unverzichtbar, um gezielt mit relevanten Daten zu arbeiten und unn\u00f6tige Informationen auszuschliessen.</p>"},{"location":"data-engineering/column_manipulations/#zeilen-mit-iloc-auswahlen-positionsbasiert","title":"Zeilen mit iloc ausw\u00e4hlen (positionsbasiert)","text":"<p>Die <code>iloc</code>-Methode ist ein leistungsstarkes Werkzeug zum Zugriff auf Daten basierend auf ihrer numerischen Position. Dies ist besonders n\u00fctzlich, wenn du mit bestimmten Zeilen und Spalten unabh\u00e4ngig von ihren Labels arbeiten m\u00f6chtest.</p> <pre><code># Erste Zeile (Index 0) - gibt eine Series zur\u00fcck\nerste_zeile = sensordaten.iloc[0]\nprint(erste_zeile)\n\n# Die ersten 3 Zeilen (Index 0, 1, 2) - gibt einen DataFrame zur\u00fcck\nerste_drei = sensordaten.iloc[0:3]  # Ende ist exklusiv!\nprint(erste_drei)\n\n# Bestimmte Zellen: Zeile 0, Spalte 1 - gibt einen einzelnen Wert zur\u00fcck\nzelle = sensordaten.iloc[0, 1]  # Erste Zeile, zweite Spalte\nprint(zelle)\n\n# Bereich von Zellen: Zeilen 0-2, Spalten 0-1 - gibt einen DataFrame zur\u00fcck\nbereich = sensordaten.iloc[0:3, 0:2]  # Die ersten 3 Zeilen, die ersten 2 Spalten\nprint(bereich)\n</code></pre> <p>Bei <code>iloc</code> ist zu beachten, dass die Indizierung bei 0 beginnt und der Endindex nicht eingeschlossen ist (wie in Python \u00fcblich).</p>"},{"location":"data-engineering/column_manipulations/#zeilen-filtern-mit-bedingungen","title":"Zeilen filtern mit Bedingungen","text":"<p>Das Filtern von Zeilen basierend auf Bedingungen ist eine der h\u00e4ufigsten Operationen in der Datenanalyse. Pandas macht es einfach, komplexe Filterbedingungen zu formulieren und anzuwenden.</p> <pre><code># Einfache Filterbedingung - w\u00e4hlt nur Zeilen aus, wo die Bedingung erf\u00fcllt ist\nhohe_temp = sensordaten[sensordaten['temp'] &gt; 25]  # Nur hohe Temperaturen\nprint(hohe_temp)\n\n# Mehrere Bedingungen kombinieren mit logischen Operatoren (&amp; f\u00fcr UND, | f\u00fcr ODER)\n# WICHTIG: Klammern um jede Bedingung setzen!\nwarm_und_trocken = sensordaten[(sensordaten['temp'] &gt; 25) &amp; (sensordaten['feuchtigkeit'] &lt; 40)]\nprint(warm_und_trocken)\n\n# ODER-Verkn\u00fcpfung\nwarm_oder_trocken = sensordaten[(sensordaten['temp'] &gt; 25) | (sensordaten['feuchtigkeit'] &lt; 40)]\nprint(warm_oder_trocken)\n\n# Filtern basierend auf einer Liste von Werten mit isin()\nbestimmte_kategorien = sensordaten[sensordaten['temp_kategorie'].isin(['Warm', 'K\u00fchl'])]\nprint(bestimmte_kategorien)\n</code></pre> <p>Die Klammern um jede Teilbedingung sind bei komplexen Filtern wichtig, da sonst Python-Operatorpr\u00e4zedenzen zu unerwarteten Ergebnissen f\u00fchren k\u00f6nnen.</p>"},{"location":"data-engineering/data_formats/","title":"Einf\u00fchrung in Datenformate und deren Einlesen in Pandas","text":"<p>Pandas unterst\u00fctzt eine Vielzahl von Datenformaten. In diesem Abschnitt zeigen wir, wie man verschiedene Formate einliest, welche Encoding-Probleme auftreten k\u00f6nnen und wie man sie handhabt.</p>"},{"location":"data-engineering/data_formats/#was-ist-encoding","title":"Was ist Encoding?","text":"<p>Encoding (Zeichenkodierung) bestimmt, wie Buchstaben und Sonderzeichen als Bytes im Computer gespeichert werden. Die h\u00e4ufigsten Encodings sind:</p> <ul> <li>UTF-8: Moderner Standard, der alle internationalen Zeichen unterst\u00fctzt</li> <li>ISO-8859-1 (auch Latin-1): \u00c4lteres Encoding f\u00fcr westeurop\u00e4ische Sprachen</li> <li>ASCII: Einfaches Encoding f\u00fcr englische Zeichen ohne Umlaute</li> </ul> <p>Falsche Encodings f\u00fchren zu Problemen wie <code>UnicodeDecodeError</code> oder unlesbaren Zeichen (z.B. <code>\u00c3\u00a4</code> statt <code>\u00e4</code>).</p>"},{"location":"data-engineering/data_formats/#einlesen-verschiedener-dateiformate","title":"Einlesen verschiedener Dateiformate","text":"<p>Pandas bietet einheitliche Funktionen zum Einlesen verschiedener Dateiformate:</p> <pre><code># CSV-Dateien einlesen\ndf = pd.read_csv(\"daten.csv\", encoding=\"utf-8\")\n\n# Excel-Dateien einlesen\ndf = pd.read_excel(\"daten.xlsx\", sheet_name=\"Tabelle1\")\n\n# JSON-Dateien einlesen\ndf = pd.read_json(\"daten.json\", encoding=\"utf-8\")\n\n# Parquet-Dateien einlesen\ndf = pd.read_parquet(\"daten.parquet\")\n</code></pre> <p>Bei Encoding-Problemen kann ein alternatives Encoding verwendet werden: <pre><code># Alternative Encodings bei Problemen mit CSV oder JSON\ndf = pd.read_csv(\"daten.csv\", encoding=\"ISO-8859-1\")\ndf = pd.read_json(\"daten.json\", encoding=\"ISO-8859-1\")\n</code></pre></p>"},{"location":"data-engineering/data_formats/#daten-in-verschiedenen-formaten-speichern","title":"Daten in verschiedenen Formaten speichern","text":"<p>Pandas erm\u00f6glicht es auch, DataFrames in verschiedenen Formaten zu speichern:</p> <pre><code># CSV-Datei speichern\ndf.to_csv(\"ausgabe.csv\", index=False)\n\n# Excel-Datei speichern\ndf.to_excel(\"ausgabe.xlsx\", index=False)\n\n# JSON-Datei speichern\ndf.to_json(\"ausgabe.json\", orient=\"records\", lines=True)\n\n# Parquet-Datei speichern\ndf.to_parquet(\"ausgabe.parquet\")\n</code></pre>"},{"location":"data-engineering/data_formats/#praktische-tipps-zum-umgang-mit-encodings","title":"Praktische Tipps zum Umgang mit Encodings","text":"<ul> <li>Dateien ansehen: Nur die ersten Zeilen einlesen mit <code>nrows=5</code></li> <li>Trennzeichen anpassen: Bei CSV mit <code>sep=\";\"</code> oder <code>delimiter=\"\\t\"</code></li> <li>Encodingprobleme umgehen: Wenn unbekannt, teste zuerst <code>utf-8</code>, dann <code>ISO-8859-1</code></li> <li>Fehlende Werte: Mit <code>na_values=[\"NA\", \"-\", \"?\"]</code> definieren</li> </ul> <pre><code># Praktisches Beispiel mit mehreren Parametern\ndf = pd.read_csv(\"daten.csv\", \n                encoding=\"utf-8\",\n                sep=\";\",\n                decimal=\",\",\n                na_values=[\"NA\", \"n.a.\"],\n                nrows=1000)\n</code></pre>"},{"location":"data-engineering/data_types/","title":"\u00dcberblick \u00fcber Datentypen","text":"<p>Pandas bietet eine Vielzahl von Datentypen, die speziell f\u00fcr die Verarbeitung und Analyse von Daten optimiert sind. Die Wahl des richtigen Datentyps kann die Speicher- und Rechenleistung erheblich beeinflussen.</p>"},{"location":"data-engineering/data_types/#1-numerische-datentypen","title":"1. Numerische Datentypen","text":""},{"location":"data-engineering/data_types/#ganzzahlige-datentypen-integer","title":"Ganzzahlige Datentypen (Integer)","text":"<p>Pandas unterst\u00fctzt verschiedene Integer-Typen mit unterschiedlicher Speichergr\u00f6sse:</p> <ul> <li>int8, int16, int32, int64: Diese Typen speichern ganze Zahlen mit 8, 16, 32 oder 64 Bit. Je gr\u00f6sser die Bitanzahl, desto gr\u00f6ssere Werte k\u00f6nnen gespeichert werden.</li> <li>Beispiel:   <pre><code>df[\"A\"] = df[\"A\"].astype(\"int32\")\n</code></pre>   Hier wird die Spalte \"A\" in den Integer-Typ mit 32 Bit umgewandelt, was Speicherplatz sparen kann, wenn keine gr\u00f6sseren Werte ben\u00f6tigt werden.</li> </ul>"},{"location":"data-engineering/data_types/#gleitkommazahlen-float","title":"Gleitkommazahlen (Float)","text":"<p>F\u00fcr Dezimalzahlen stehen folgende Datentypen zur Verf\u00fcgung:</p> <ul> <li>float16, float32, float64: Diese Typen repr\u00e4sentieren Gleitkommazahlen mit 16, 32 oder 64 Bit. H\u00f6here Bitwerte bieten eine h\u00f6here Genauigkeit, verbrauchen jedoch mehr Speicher.</li> <li>Beispiel:   <pre><code>df[\"B\"] = df[\"B\"].astype(\"float64\")\n</code></pre>   In diesem Beispiel wird die Spalte \"B\" in den Standard-Gleitkommatyp mit 64 Bit konvertiert.</li> </ul>"},{"location":"data-engineering/data_types/#2-boolesche-werte","title":"2. Boolesche Werte","text":"<p>Der boolesche Datentyp speichert Wahrheitswerte:</p> <ul> <li>bool: Kann nur die Werte <code>True</code> oder <code>False</code> annehmen.</li> <li>Beispiel:   <pre><code>df[\"C\"] = df[\"C\"].astype(\"bool\")\n</code></pre>   Damit wird die Spalte \"C\" in den Datentyp <code>bool</code> umgewandelt, wodurch nur zwei m\u00f6gliche Werte gespeichert werden k\u00f6nnen.</li> </ul>"},{"location":"data-engineering/data_types/#3-zeichenketten-strings","title":"3. Zeichenketten (Strings)","text":"<p>Pandas speichert Zeichenketten standardm\u00e4ssig als <code>object</code>, bietet aber auch eine optimierte Variante:</p> <ul> <li>object: Allgemeiner Typ f\u00fcr gemischte Daten, meist f\u00fcr Strings verwendet.</li> <li>string: Ein effizienterer String-Datentyp, eingef\u00fchrt in neueren Pandas-Versionen.</li> <li>Beispiel:   <pre><code>df[\"D\"] = df[\"D\"].astype(\"string\")\n</code></pre>   Hier wird die Spalte \"D\" explizit in den neuen <code>string</code>-Typ umgewandelt, um Speicherplatz zu sparen und die Verarbeitung zu beschleunigen.</li> </ul>"},{"location":"data-engineering/data_types/#4-kategorische-daten","title":"4. Kategorische Daten","text":"<p>Der <code>category</code>-Datentyp eignet sich f\u00fcr Spalten mit wiederkehrenden Werten:</p> <ul> <li>category: Spart Speicherplatz, indem wiederholte Werte als numerische Kategorien gespeichert werden.</li> <li>Beispiel:   <pre><code>df[\"E\"] = df[\"E\"].astype(\"category\")\n</code></pre>   Diese Umwandlung ist n\u00fctzlich f\u00fcr Spalten mit wenigen einzigartigen Werten, wie zum Beispiel Geschlechter oder Produktkategorien.</li> </ul>"},{"location":"data-engineering/data_types/#5-zeitbezogene-datentypen","title":"5. Zeitbezogene Datentypen","text":""},{"location":"data-engineering/data_types/#datums-und-zeitstempel","title":"Datums- und Zeitstempel","text":"<p>F\u00fcr die Arbeit mit Zeitstempeln gibt es den Datentyp:</p> <ul> <li>datetime64[ns]: Speichert Datums- und Uhrzeitangaben mit Nanosekunden-Pr\u00e4zision.</li> <li>Beispiel:   <pre><code>df[\"F\"] = pd.to_datetime(df[\"F\"])\n</code></pre>   Hier wird sichergestellt, dass die Spalte \"F\" korrekt als Zeitstempel interpretiert wird.</li> </ul>"},{"location":"data-engineering/data_types/#zeitdifferenzen","title":"Zeitdifferenzen","text":"<p>Pandas erm\u00f6glicht auch die Berechnung von Zeitspannen:</p> <ul> <li>timedelta64[ns]: Speichert die Differenz zwischen zwei Zeitstempeln.</li> <li>Beispiel:   <pre><code>df[\"G\"] = df[\"H\"] - df[\"I\"]\n</code></pre>   In diesem Fall wird die Differenz zwischen den Spalten \"H\" und \"I\" berechnet und in einer neuen Spalte \"G\" gespeichert.</li> </ul>"},{"location":"data-engineering/drop_duplicates/","title":"Entfernen von Duplikaten","text":"<p>In der Arbeit mit Sensordaten k\u00f6nnen durch fehlerhafte Erfassung oder doppelte \u00dcbertragung Duplikate entstehen. Die Methode <code>drop_duplicates</code> von pandas erm\u00f6glicht es, solche Duplikate effizient zu entfernen und so die Daten konsistenter zu gestalten.  </p> <p>Hauptparameter f\u00fcr Sensordaten: </p> <ul> <li>subset: Gibt die Spalten an, auf deren Basis Duplikate gepr\u00fcft werden sollen, z. B. Zeitstempel oder Sensormessungen.  </li> <li>keep: Bestimmt, welches Duplikat beibehalten wird (<code>'first'</code>, <code>'last'</code> oder <code>False</code> f\u00fcr keines).  </li> </ul>"},{"location":"data-engineering/drop_duplicates/#beispiel-1-duplikate-basierend-auf-zeitstempeln-entfernen","title":"Beispiel 1: Duplikate basierend auf Zeitstempeln entfernen","text":"<p>Doppelte Eintr\u00e4ge in Sensordaten k\u00f6nnen vorkommen, z. B. wenn ein Sensor denselben Wert mehrfach sendet.  </p> <pre><code>import pandas as pd\n\n# Beispiel-Sensordaten mit Duplikaten\ndata = {\n    'Zeitstempel': ['2025-02-01 12:00', '2025-02-01 12:10', '2025-02-01 12:10', '2025-02-01 12:20'],\n    'Temperatur': [22.5, 23.0, 23.0, 24.5]\n}\ndf = pd.DataFrame(data)\ndf['Zeitstempel'] = pd.to_datetime(df['Zeitstempel'])\n\nprint(\"Originale Sensordaten:\")\nprint(df)\n\n# Duplikate basierend auf dem Zeitstempel entfernen\ndf_cleaned = df.drop_duplicates(subset='Zeitstempel', keep='first')\n\nprint(\"\\nNach Entfernen der Duplikate basierend auf dem Zeitstempel:\")\nprint(df_cleaned)\n</code></pre> <p>Ausgabe: <pre><code>Originale Sensordaten:\n          Zeitstempel  Temperatur\n0 2025-02-01 12:00:00        22.5\n1 2025-02-01 12:10:00        23.0\n2 2025-02-01 12:10:00        23.0\n3 2025-02-01 12:20:00        24.5\n\nNach Entfernen der Duplikate basierend auf dem Zeitstempel:\n          Zeitstempel  Temperatur\n0 2025-02-01 12:00:00        22.5\n1 2025-02-01 12:10:00        23.0\n3 2025-02-01 12:20:00        24.5\n</code></pre></p>"},{"location":"data-engineering/drop_duplicates/#beispiel-2-duplikate-vollstandig-entfernen","title":"Beispiel 2: Duplikate vollst\u00e4ndig entfernen","text":"<p>Wenn Sensoren fehlerhafte oder doppelte Eintr\u00e4ge senden, k\u00f6nnen diese komplett entfernt werden, um eine genaue Analyse sicherzustellen.  </p> <pre><code># Alle doppelten Eintr\u00e4ge entfernen (keine Eintr\u00e4ge behalten)\ndf_cleaned = df.drop_duplicates(keep=False)\n\nprint(\"\\nNach vollst\u00e4ndigem Entfernen der Duplikate:\")\nprint(df_cleaned)\n</code></pre> <p>Ausgabe: <pre><code>Nach vollst\u00e4ndigem Entfernen der Duplikate:\n          Zeitstempel  Temperatur\n0 2025-02-01 12:00:00        22.5\n3 2025-02-01 12:20:00        24.5\n</code></pre></p>"},{"location":"data-engineering/fillna/","title":"Bef\u00fcllen von fehlenden Werten","text":"<p>In der Arbeit mit Sensordaten treten oft L\u00fccken in den Datens\u00e4tzen auf, da Sensoren gelegentlich keine Messwerte liefern k\u00f6nnen. Diese fehlenden Werte (<code>NaN</code>) m\u00fcssen h\u00e4ufig bereinigt werden, um eine korrekte Analyse und Modellierung zu erm\u00f6glichen. Die Methode <code>fillna</code> von pandas bietet eine effektive M\u00f6glichkeit, solche L\u00fccken zu schliessen, indem fehlende Werte durch feste Werte, statistische Kennzahlen (z. B. Mittelwert) oder eine F\u00fcllstrategie wie Vorw\u00e4rts- oder R\u00fcckw\u00e4rtsf\u00fcllung ersetzt werden.</p> <p>F\u00fcr Sensordaten ist diese Methode besonders n\u00fctzlich, um:</p> <ul> <li>Zeitreihen zu gl\u00e4tten und kontinuierliche Daten sicherzustellen.</li> <li>Fehlende Werte auf Basis historischer Daten sinnvoll zu ersetzen.</li> <li>Daten f\u00fcr maschinelles Lernen oder weitere Analysen vorzubereiten.</li> </ul> <p>Hauptparameter f\u00fcr Sensordaten:</p> <ul> <li>value: Feste Werte, z. B. <code>0</code>, falls ein Sensorfehler oder ein erwartetes physikalisches Verhalten dies nahelegt.</li> <li>method: Vorw\u00e4rtsf\u00fcllung (<code>method='ffill'</code>) oder R\u00fcckw\u00e4rtsf\u00fcllung (<code>method='bfill'</code>), um Messl\u00fccken basierend auf benachbarten Werten zu schliessen.</li> </ul>"},{"location":"data-engineering/fillna/#beispiel-1-vorwartsfullung-bei-kontinuierlichen-sensordaten","title":"Beispiel 1: Vorw\u00e4rtsf\u00fcllung bei kontinuierlichen Sensordaten","text":"<p>Ein Anwendungsfall, bei dem Vorw\u00e4rtsf\u00fcllung sinnvoll ist, ist die Messung von Temperatur- oder Feuchtigkeitswerten, bei denen der letzte gemessene Wert bis zur n\u00e4chsten Messung g\u00fcltig bleibt.</p> <pre><code>import pandas as pd\n\n# Beispiel f\u00fcr Sensordaten mit fehlenden Werten\ndata = {\n    'Temperatur': [22.5, 23.0, None, 24.5, 25.0],\n    'Feuchtigkeit': [55.0, None, 60.0, None, 65.0]\n}\ntimestamps = ['2025-02-01 12:00', '2025-02-01 12:10', '2025-02-01 12:20', '2025-02-01 12:30', '2025-02-01 12:40']\ndf = pd.DataFrame(data, index=pd.to_datetime(timestamps))\n\nprint(\"Originale Sensordaten:\")\nprint(df)\n\n# Fehlende Werte durch Vorw\u00e4rtsf\u00fcllung schliessen\ndf_filled = df.fillna(method='ffill')\n\nprint(\"\\nNach Vorw\u00e4rtsf\u00fcllung der fehlenden Werte:\")\nprint(df_filled)\n</code></pre> <p>Ausgabe: <pre><code>Originale Sensordaten:\n                     Temperatur  Feuchtigkeit\n2025-02-01 12:00:00        22.5          55.0\n2025-02-01 12:10:00        23.0           NaN\n2025-02-01 12:20:00         NaN          60.0\n2025-02-01 12:30:00        24.5           NaN\n2025-02-01 12:40:00        25.0          65.0\n\nNach Vorw\u00e4rtsf\u00fcllung der fehlenden Werte:\n                     Temperatur  Feuchtigkeit\n2025-02-01 12:00:00        22.5          55.0\n2025-02-01 12:10:00        23.0          55.0\n2025-02-01 12:20:00        23.0          60.0\n2025-02-01 12:30:00        24.5          60.0\n2025-02-01 12:40:00        25.0          65.0\n</code></pre></p>"},{"location":"data-engineering/fillna/#beispiel-2-fullen-mit-0-bei-solarpanel-daten","title":"Beispiel 2: F\u00fcllen mit <code>0</code> bei Solarpanel-Daten","text":"<p>In manchen F\u00e4llen ist es sinnvoll, fehlende Werte mit <code>0</code> zu ersetzen, da sie ein erwartetes physikalisches Verhalten widerspiegeln. Zum Beispiel produziert ein Solarpanel in der Nacht keinen Strom, weshalb <code>NaN</code>-Werte durch <code>0</code> ersetzt werden k\u00f6nnen.</p> <pre><code>import pandas as pd\n\n# Beispiel f\u00fcr Sensordaten zur Stromproduktion von Solarpanels\ndata = {\n    'Solarstrom (kW)': [None, 0.0, 5.0, 10.0, None, None, 0.0, None]\n}\ntimestamps = [\n    '2025-02-01 00:00', '2025-02-01 06:00', '2025-02-01 10:00', \n    '2025-02-01 12:00', '2025-02-01 16:00', '2025-02-01 18:00', \n    '2025-02-01 20:00', '2025-02-01 23:59'\n]\ndf = pd.DataFrame(data, index=pd.to_datetime(timestamps))\n\nprint(\"Originale Sensordaten:\")\nprint(df)\n\n# Fehlende Werte durch 0 ersetzen, da nachts keine Stromproduktion stattfindet\ndf_filled = df.fillna(value=0)\n\nprint(\"\\nNach F\u00fcllen der fehlenden Werte mit 0:\")\nprint(df_filled)\n</code></pre> <p>Ausgabe: <pre><code>Originale Sensordaten:\n                     Solarstrom (kW)\n2025-02-01 00:00:00              NaN\n2025-02-01 06:00:00              0.0\n2025-02-01 10:00:00              5.0\n2025-02-01 12:00:00             10.0\n2025-02-01 16:00:00              NaN\n2025-02-01 18:00:00              NaN\n2025-02-01 20:00:00              0.0\n2025-02-01 23:59:00              NaN\n\nNach F\u00fcllen der fehlenden Werte mit 0:\n                     Solarstrom (kW)\n2025-02-01 00:00:00              0.0\n2025-02-01 06:00:00              0.0\n2025-02-01 10:00:00              5.0\n2025-02-01 12:00:00             10.0\n2025-02-01 16:00:00              0.0\n2025-02-01 18:00:00              0.0\n2025-02-01 20:00:00              0.0\n2025-02-01 23:59:00              0.0\n</code></pre></p> <p>In diesem Beispiel wurden <code>NaN</code>-Werte durch <code>0</code> ersetzt, da nachts keine Stromproduktion stattfindet. Diese Vorgehensweise hilft, die Daten logisch korrekt und f\u00fcr Analysen konsistent zu halten.</p>"},{"location":"data-engineering/imputation/","title":"Imputation in Pandas","text":"<p>Fehlende Werte in Datens\u00e4tzen sind ein h\u00e4ufiges Problem in der Datenanalyse. Sie k\u00f6nnen durch Messfehler, nicht beantwortete Fragen in Umfragen oder technische Probleme entstehen. Die Imputation (Ersetzung) fehlender Werte kann durch verschieden komplexe Methoden erfolgen (wie Durchschnitt oder lineare Interpolation).</p>"},{"location":"data-engineering/imputation/#grundlegende-imputationsmethoden","title":"Grundlegende Imputationsmethoden","text":"<p>Die einfachste Form der Imputation ist das Ersetzen fehlender Werte durch einen konstanten Wert. Diese ist in Entfernen von Ausreissern schon beschrieben.</p>"},{"location":"data-engineering/imputation/#fortgeschrittene-imputationsmethoden","title":"Fortgeschrittene Imputationsmethoden","text":"<p>F\u00fcr Zeitreihendaten bietet Pandas verschiedene Interpolationsmethoden an. Mit einer linearen Interpolation werden die Daten zwischen den n\u00e4chsten existierenden Werten durch eine gerade Linie interpoliert. </p> <pre><code>import pandas as pd\n\ndata = {\n    'Zeitstempel': ['2025-02-01 08:00', '2025-02-01 09:00', '2025-02-01 10:00', \n                    '2025-02-01 11:00', '2025-02-01 12:00', '2025-02-01 13:00'],\n    'Temperatur': [21.5, 22.0, None, None, 24.5, 25.0]\n}\n\ndf = pd.DataFrame(data)\ndf['Zeitstempel'] = pd.to_datetime(df['Zeitstempel'])\ndf = df.set_index('Zeitstempel')\n\nprint(\"Originale Zeitreihendaten mit fehlenden Werten:\")\nprint(df)\n\n# Lineare Interpolation\ndf_linear = df.interpolate(method='linear')\n\nprint(\"\\nDataFrame nach linearer Interpolation:\")\nprint(df_linear)\n</code></pre> <p>Ausgabe: <pre><code>Originale Zeitreihendaten mit fehlenden Werten:\n                     Temperatur\nZeitstempel                    \n2025-02-01 08:00:00        21.5\n2025-02-01 09:00:00        22.0\n2025-02-01 10:00:00         NaN\n2025-02-01 11:00:00         NaN\n2025-02-01 12:00:00        24.5\n2025-02-01 13:00:00        25.0\n\nDataFrame nach linearer Interpolation:\n                     Temperatur\nZeitstempel                    \n2025-02-01 08:00:00   21.500000\n2025-02-01 09:00:00   22.000000\n2025-02-01 10:00:00   22.833333\n2025-02-01 11:00:00   23.666667\n2025-02-01 12:00:00   24.500000\n2025-02-01 13:00:00   25.000000\n</code></pre></p>"},{"location":"data-engineering/introduction/","title":"Einf\u00fchrung in Pandas","text":"<p>Pandas ist eine leistungsstarke Bibliothek f\u00fcr die Datenanalyse in Python. Sie bietet effiziente Werkzeuge zur Verarbeitung, Filterung und Aggregation von Daten.</p>"},{"location":"data-engineering/introduction/#1-installation-und-import","title":"1. Installation und Import","text":"<p>Falls Pandas noch nicht installiert ist, kann es mit folgendem Befehl installiert werden: <pre><code>pip install pandas\n</code></pre></p> <p>Der Import erfolgt standardm\u00e4ssig so: <pre><code>import pandas as pd\n</code></pre></p>"},{"location":"data-engineering/introduction/#2-einlesen-von-daten","title":"2. Einlesen von Daten","text":""},{"location":"data-engineering/introduction/#csv-datei-einlesen","title":"CSV-Datei einlesen","text":"<p>Eine der h\u00e4ufigsten Methoden zum Einlesen von Daten ist das Laden einer CSV-Datei: <pre><code>df = pd.read_csv(\"daten.csv\")\n</code></pre></p>"},{"location":"data-engineering/introduction/#excel-datei-einlesen","title":"Excel-Datei einlesen","text":"<p>Falls die Daten in einer Excel-Datei gespeichert sind: <pre><code>df = pd.read_excel(\"daten.xlsx\")\n</code></pre></p>"},{"location":"data-engineering/introduction/#3-anzeige-von-daten","title":"3. Anzeige von Daten","text":""},{"location":"data-engineering/introduction/#die-ersten-zeilen-anzeigen","title":"Die ersten Zeilen anzeigen","text":"<p>Um einen schnellen \u00dcberblick \u00fcber die ersten f\u00fcnf Zeilen des DataFrames zu erhalten: <pre><code>df.head()\n</code></pre></p>"},{"location":"data-engineering/introduction/#die-letzten-zeilen-anzeigen","title":"Die letzten Zeilen anzeigen","text":"<p>Falls die letzten Zeilen betrachtet werden sollen: <pre><code>df.tail()\n</code></pre></p>"},{"location":"data-engineering/introduction/#grundlegende-informationen-uber-den-dataframe","title":"Grundlegende Informationen \u00fcber den DataFrame","text":"<p>Um eine \u00dcbersicht \u00fcber Spalten, Datentypen und fehlende Werte zu bekommen: <pre><code>df.info()\n</code></pre></p>"},{"location":"data-engineering/introduction/#statistische-kennzahlen-anzeigen","title":"Statistische Kennzahlen anzeigen","text":"<p>F\u00fcr eine schnelle statistische Analyse numerischer Spalten: <pre><code>df.describe()\n</code></pre></p>"},{"location":"data-engineering/introduction/#4-daten-filtern","title":"4. Daten filtern","text":""},{"location":"data-engineering/introduction/#zeilen-basierend-auf-einer-bedingung-auswahlen","title":"Zeilen basierend auf einer Bedingung ausw\u00e4hlen","text":"<p>Um nur Zeilen anzuzeigen, bei denen eine bestimmte Spalte einen bestimmten Wert hat: <pre><code>df[df[\"Spalte\"] == \"Wert\"]\n</code></pre></p>"},{"location":"data-engineering/introduction/#mehrere-bedingungen-kombinieren","title":"Mehrere Bedingungen kombinieren","text":"<pre><code>df[(df[\"Alter\"] &gt; 30) &amp; (df[\"Stadt\"] == \"Z\u00fcrich\")]\n</code></pre>"},{"location":"data-engineering/introduction/#filtern-mit-der-query-funktion","title":"Filtern mit der <code>query</code>-Funktion","text":"<p>Die <code>query</code>-Methode erm\u00f6glicht eine elegante und lesbare Filterung des DataFrames: <pre><code>df.query(\"Alter &gt; 30 and Stadt == 'Z\u00fcrich'\")\n</code></pre> Diese Methode eignet sich besonders f\u00fcr komplexe Filterabfragen und verbessert die Lesbarkeit des Codes.</p>"},{"location":"data-engineering/introduction/#5-aggregationen-auf-spalten","title":"5. Aggregationen auf Spalten","text":""},{"location":"data-engineering/introduction/#berechnungen-auf-eine-einzelne-spalte-anwenden","title":"Berechnungen auf eine einzelne Spalte anwenden","text":"<p>Summe aller Werte einer Spalte: <pre><code>df[\"Umsatz\"].sum()\n</code></pre></p> <p>Durchschnittswert einer Spalte: <pre><code>df[\"Preis\"].mean()\n</code></pre></p>"},{"location":"data-engineering/introduction/#6-sampling-von-daten","title":"6. Sampling von Daten","text":""},{"location":"data-engineering/introduction/#zufallige-stichprobe-ziehen","title":"Zuf\u00e4llige Stichprobe ziehen","text":"<p>Eine zuf\u00e4llige Stichprobe von n Zeilen aus dem DataFrame ziehen: <pre><code>df.sample(n=10)\n</code></pre></p>"},{"location":"data-engineering/introduction/#zufallige-stichprobe-mit-einem-bestimmten-anteil-der-daten","title":"Zuf\u00e4llige Stichprobe mit einem bestimmten Anteil der Daten","text":"<p>Um z.B. 20% der Zeilen zuf\u00e4llig auszuw\u00e4hlen: <pre><code>df.sample(frac=0.2)\n</code></pre></p>"},{"location":"data-engineering/introduction/#stichprobe-mit-einer-festen-zufallsgenerierung","title":"Stichprobe mit einer festen Zufallsgenerierung","text":"<p>Falls man reproduzierbare Zufallsauswahlen treffen m\u00f6chte, kann ein Zufallszahlengenerator gesetzt werden: <pre><code>df.sample(n=10, random_state=42)\n</code></pre></p>"},{"location":"data-engineering/joins/","title":"Joins in Pandas","text":"<p>In der Praxis liegen Daten h\u00e4ufig in verschiedenen Tabellen oder Datenquellen vor. Joins sind notwendig, um diese verschiedenen Datenquellen sinnvoll zu kombinieren.</p>"},{"location":"data-engineering/joins/#grundlegende-join-typen","title":"Grundlegende Join-Typen","text":""},{"location":"data-engineering/joins/#inner-join","title":"Inner Join","text":"<p>Der Inner Join kombiniert nur die Zeilen, die in beiden DataFrames \u00fcbereinstimmende (Zeit-)Schl\u00fcssel haben.</p> <pre><code>import pandas as pd\n\n# Erstellen von Beispiel-DataFrames mit Zeitreihendaten\ndates_temp = pd.date_range('2025-01-01', '2025-01-07') # freq = 15min\ntemp_values = [22.0, 20.7, 19.7, 20.5, 19.5, 20.7, 19.0]\ndf_temp = pd.DataFrame({'Temperature': temp_values}, index=dates_temp)\ndf_temp.index.name = 'Date'\n\ndates_co2 = pd.date_range('2025-01-03', '2025-01-09')\nco2_values = [502.7, 487.3, 456.6, 453.5, 508.9, 772.1, 745.8]\ndf_co2 = pd.DataFrame({'CO2': co2_values}, index=dates_co2)\ndf_co2.index.name = 'Date'\n\nprint(\"Temperature Data:\")\nprint(df_temp)\nprint(\"\\nCO2 Data:\")\nprint(df_co2)\n\n# Inner Join - nur \u00fcbereinstimmende Daten\ndf_inner = df_temp.join(df_co2, how='inner')\nprint(\"\\nInner Join Ergebnis:\")\nprint(df_inner)\n</code></pre> <p>Ausgabe: <pre><code>Temperature Data:\n            Temperature\nDate                   \n2025-01-01         22.0\n2025-01-02         20.7\n2025-01-03         19.7\n2025-01-04         20.5\n2025-01-05         19.5\n2025-01-06         20.7\n2025-01-07         19.0\n\nCO2 Data:\n              CO2\nDate             \n2025-01-03  502.7\n2025-01-04  487.3\n2025-01-05  456.6\n2025-01-06  453.5\n2025-01-07  508.9\n2025-01-08  772.1\n2025-01-09  745.8\n\nInner Join Ergebnis:\n            Temperature    CO2\nDate                          \n2025-01-03         19.7  502.7\n2025-01-04         20.5  487.3\n2025-01-05         19.5  456.6\n2025-01-06         20.7  453.5\n2025-01-07         19.0  508.9\n</code></pre></p>"},{"location":"data-engineering/joins/#left-join","title":"Left Join","text":"<p>Der Left Join beh\u00e4lt alle Zeilen aus dem linken DataFrame und f\u00fcgt passende Daten aus dem rechten DataFrame hinzu.</p> <pre><code>df_left = df_temp.join(df_co2, how='left')\nprint(\"\\nLeft Join Ergebnis:\")\nprint(df_left)\n</code></pre> <pre><code>Left Join Ergebnis:\n            Temperature    CO2\nDate                          \n2025-01-01         22.0    NaN\n2025-01-02         20.7    NaN\n2025-01-03         19.7  502.7\n2025-01-04         20.5  487.3\n2025-01-05         19.5  456.6\n2025-01-06         20.7  453.5\n2025-01-07         19.0  508.9\n</code></pre>"},{"location":"data-engineering/joins/#right-join","title":"Right Join","text":"<p>Der Right Join beh\u00e4lt alle Zeilen aus dem rechten DataFrame und f\u00fcgt passende Daten aus dem linken DataFrame hinzu.</p> <pre><code>df_right = df_temp.join(df_co2, how='right')\nprint(\"\\nRight Join Ergebnis:\")\nprint(df_right)\n</code></pre> <pre><code>Right Join Ergebnis:\n            Temperature    CO2\nDate                          \n2025-01-03         19.7  502.7\n2025-01-04         20.5  487.3\n2025-01-05         19.5  456.6\n2025-01-06         20.7  453.5\n2025-01-07         19.0  508.9\n2025-01-08          NaN  772.1\n2025-01-09          NaN  745.8\n</code></pre>"},{"location":"data-engineering/joins/#outer-join","title":"Outer Join","text":"<p>Der Outer Join (Full Outer Join) beh\u00e4lt alle Zeilen aus beiden DataFrames.</p> <pre><code>df_outer = df_temp.join(df_co2, how='outer')\nprint(\"\\nOuter Join Ergebnis:\")\nprint(df_outer)\n</code></pre> <pre><code>Outer Join Ergebnis:\n            Temperature    CO2\nDate                          \n2025-01-01         22.0    NaN\n2025-01-02         20.7    NaN\n2025-01-03         19.7  502.7\n2025-01-04         20.5  487.3\n2025-01-05         19.5  456.6\n2025-01-06         20.7  453.5\n2025-01-07         19.0  508.9\n2025-01-08          NaN  772.1\n2025-01-09          NaN  745.8\n</code></pre>"},{"location":"data-engineering/joins/#join-mit-einem-generierten-zeitindex","title":"Join mit einem generierten Zeitindex","text":"<p>Es kann n\u00fctzlich sein, das eigene Datenframe mit einem neuen generierten Zeitindex zu joinen, um herauszufinden an welchen Tagen es fehlende Werte gibt. Dies ist besonders n\u00fctzlich bei grossen Datenframes, da man dort schnell die \u00dcbersicht verliert. Dies wird erreicht indem eine neue Date-Range erstellt mit dem Start und Endwert des aktuellen Datenframes und diese beide dann wie gewohnt joined werden.</p> <pre><code>import pandas as pd\n\n# Erstellen von Beispiel-DataFrames mit Zeitreihendaten\ndates_temp = pd.date_range('2025-01-01', '2025-01-07')\ntemp_values = [22.0, 20.7, 19.7, 20.5, 19.5, 20.7, 19.0]\ndf_temp = pd.DataFrame({'Temperature': temp_values}, index=dates_temp)\ndf_temp.index.name = 'Date'\n\n# Fehlender Wert hinzuf\u00fcgen\ndf_temp = df_temp[df_temp.index != pd.to_datetime('2025-01-05')]\n\n# Neues Datenframe mit k\u00fcnstlichem Zeitindex erstellen\ntime_idx = pd.date_range(start=df_temp.index.min(), end=df_temp.index.max(), freq='1D')\ntime_df = pd.DataFrame(index=time_idx)\n\nfull_df = df_temp.join(time_df, how=\"outer\")\n\nprint(full_df)\n\n# Nur Zeilen mit fehlenden Werten ausgeben\nprint(full_df[full_df.isna().any(axis=1)])\n</code></pre> <p>Ausgabe (Datenframe mit sichtbaren fehlenden Werten): <pre><code>            Temperature\n2025-01-01         22.0\n2025-01-02         20.7\n2025-01-03         19.7\n2025-01-04         20.5\n2025-01-05          NaN\n2025-01-06         20.7\n2025-01-07         19.0\n</code></pre></p> <p>Ausgabe (nur fehlende Werte): <pre><code>            Temperature\n2025-01-05          NaN\n</code></pre></p>"},{"location":"data-engineering/outliers/","title":"Entfernen von Ausreissern mit dem z-Score","text":"<p>In Sensordaten k\u00f6nnen Ausreisser auftreten, zum Beispiel durch fehlerhafte Messungen oder ungew\u00f6hnliche Ereignisse. Ein h\u00e4ufiger Ansatz, um solche Werte zu erkennen und zu entfernen, ist die Berechnung des z-Scores.  </p>"},{"location":"data-engineering/outliers/#was-ist-der-z-score","title":"Was ist der z-Score?","text":"<p>Der z-Score ist ein Wert, der Ihnen hilft zu verstehen, wie weit ein bestimmter Wert vom Durchschnitt der anderen Werte entfernt ist. Um dies zu berechnen, verwenden wir die Standardabweichung, ein Mass daf\u00fcr, wie stark die Werte normalerweise vom Durchschnitt abweichen.</p> <p>Die Formel f\u00fcr den z-Score lautet:</p> \\[ z = \\frac{X - \\mu}{\\sigma} \\] <p>Dabei ist:</p> <ul> <li>\\(X\\): Der Wert, den Sie untersuchen m\u00f6chten (z. B. eine gemessene Temperatur).</li> <li>\\(\\mu\\): Der Durchschnitt der Werte (also der Mittelwert, den Sie aus allen Messungen berechnen).</li> <li>\\(\\sigma\\): Die Standardabweichung, die angibt, wie stark sich die Werte vom Durchschnitt unterscheiden. Eine hohe Standardabweichung bedeutet, dass die Werte weit verstreut sind, eine niedrige bedeutet, dass sie dicht beieinander liegen.</li> </ul>"},{"location":"data-engineering/outliers/#was-ist-die-standardabweichung","title":"Was ist die Standardabweichung?","text":"<p>Die Standardabweichung ist ein Mass daf\u00fcr, wie weit die einzelnen Werte im Durchschnitt vom Mittelwert entfernt sind. Wenn alle Werte in etwa gleich dem Durchschnitt sind, ist die Standardabweichung klein. Wenn die Werte stark variieren, ist die Standardabweichung gross.</p>"},{"location":"data-engineering/outliers/#was-bedeutet-der-z-score","title":"Was bedeutet der z-Score?","text":"<p>Der z-Score zeigt Ihnen, wie weit ein Wert vom Durchschnitt entfernt ist, und gibt dabei auch eine Vorstellung davon, wie normal oder ungew\u00f6hnlich dieser Wert im Vergleich zu den anderen ist. Hier einige Beispiele:</p> <ul> <li>z-Score = 0: Der Wert liegt genau im Durchschnitt. Das bedeutet, dass der Wert keinen Abstand zum Mittelwert hat.</li> <li>z-Score = +1: Der Wert liegt 1 Standardabweichung \u00fcber dem Durchschnitt. Etwa 68% der Werte in einer normal verteilten Menge liegen innerhalb von 1 Standardabweichung vom Durchschnitt.</li> <li>z-Score = -2: Der Wert liegt 2 Standardabweichungen unter dem Durchschnitt. Ungef\u00e4hr 95% der Werte liegen innerhalb von 2 Standardabweichungen vom Durchschnitt.</li> <li>z-Score = +3: Der Wert liegt 3 Standardabweichungen \u00fcber dem Durchschnitt. Nur etwa 99.7% der Werte einer normalen Verteilung liegen innerhalb von 3 Standardabweichungen vom Durchschnitt.</li> </ul>"},{"location":"data-engineering/outliers/#warum-ist-der-z-score-nutzlich","title":"Warum ist der z-Score n\u00fctzlich?","text":"<p>Der z-Score hilft Ihnen zu erkennen, ob ein Wert normal oder ungew\u00f6hnlich ist. Ein hoher z-Score (z. B. &gt; 3) oder ein sehr niedriger z-Score (z. B. &lt; -3) deutet darauf hin, dass der Wert weit vom Durchschnitt entfernt liegt und daher als Ausreisser betrachtet werden k\u00f6nnte. Solche Werte kommen in einer normalen Verteilung nur selten vor.</p>"},{"location":"data-engineering/outliers/#ein-einfaches-beispiel","title":"Ein einfaches Beispiel:","text":"<p>Stellen Sie sich vor, Sie messen die Temperatur an verschiedenen Tagen und bekommen Werte wie: - 20\u00b0C, 22\u00b0C, 21\u00b0C, 100\u00b0C, 23\u00b0C</p> <p>Der Durchschnitt dieser Temperaturen liegt etwa bei 22\u00b0C. Wenn wir die Standardabweichung berechnen und den z-Score f\u00fcr die Temperatur von 100\u00b0C berechnen, werden wir feststellen, dass dieser Wert weit vom Durchschnitt entfernt ist. Der z-Score f\u00fcr 100\u00b0C wird sehr hoch sein, was darauf hinweist, dass dieser Wert ein Ausreisser ist. In einer normalen Verteilung w\u00fcrden Werte wie 100\u00b0C nur in weniger als 0,3% der F\u00e4lle vorkommen (mehr als 3 Standardabweichungen vom Durchschnitt entfernt).</p>"},{"location":"data-engineering/outliers/#hinweis-zu-sensordaten","title":"Hinweis zu Sensordaten:","text":"<p>Das Entfernen von Ausreissern kann die Datenqualit\u00e4t verbessern, indem Extremwerte, die m\u00f6glicherweise Fehler sind, beseitigt werden. Allerdings besteht die Gefahr, dass auch g\u00fcltige, aber seltene Ereignisse entfernt werden, was zu einem Informationsverlust f\u00fchren k\u00f6nnte.  </p>"},{"location":"data-engineering/outliers/#beispiel-entfernen-von-ausreissern-aus-temperaturdaten","title":"Beispiel: Entfernen von Ausreissern aus Temperaturdaten","text":"<pre><code>import pandas as pd\n\n# Beispiel-Sensordaten mit Ausreissern (noch mehr Werte)\ndata = {\n    'Zeitstempel': ['2025-02-01 12:00', '2025-02-01 12:10', '2025-02-01 12:20', \n                    '2025-02-01 12:30', '2025-02-01 12:40', '2025-02-01 12:50', \n                    '2025-02-01 13:00', '2025-02-01 13:10', '2025-02-01 13:20', \n                    '2025-02-01 13:30', '2025-02-01 13:40', '2025-02-01 13:50', \n                    '2025-02-01 14:00', '2025-02-01 14:10', '2025-02-01 14:20'],\n    'Temperatur': [22.5, 23.0, 100.0, 24.5, 25.0, 23.5, 24.0, 22.0, 24.5, 23.0, \n                   23.8, 24.2, 22.5, 23.1, 24.3]  # 100.0 bleibt der Ausreisser\n}\ndf = pd.DataFrame(data)\ndf['Zeitstempel'] = pd.to_datetime(df['Zeitstempel'])\n\nprint(\"Originale Sensordaten:\")\nprint(df)\n\n# Berechnung des Mittelwerts und der Standardabweichung\nmean_temp = df['Temperatur'].mean()\nstd_temp = df['Temperatur'].std()\n\n# Berechnung des z-Scores\ndf['z_score'] = (df['Temperatur'] - mean_temp) / std_temp\n\n# Entfernen von Werten mit z-Score gr\u00f6sser als 3 oder kleiner als -3\ndf_cleaned = df[df['z_score'].abs() &lt;= 3]\n\n# Entfernen der Spalte 'z_score'\ndf_cleaned = df_cleaned.drop(columns=['z_score'])\n\nprint(\"\\nNach Entfernen der Ausreisser:\")\nprint(df_cleaned)\n</code></pre> <p>Ausgabe: <pre><code>Originale Sensordaten:\n           Zeitstempel  Temperatur\n0  2025-02-01 12:00:00        22.5\n1  2025-02-01 12:10:00        23.0\n2  2025-02-01 12:20:00       100.0\n3  2025-02-01 12:30:00        24.5\n                               ...\n\nNach Entfernen der Ausreisser basierend auf dem z-Score:\n           Zeitstempel  Temperatur\n0  2025-02-01 12:00:00        22.5\n1  2025-02-01 12:10:00        23.0\n3  2025-02-01 12:30:00        24.5\n4  2025-02-01 12:40:00        25.0\n                               ...\n</code></pre></p>"},{"location":"data-engineering/outliers/#erklarung-zur-folge-des-entfernens-von-ausreissern","title":"Erkl\u00e4rung zur Folge des Entfernens von Ausreissern","text":"<p>Durch das Entfernen von Ausreissern k\u00f6nnen Sensordaten konsistenter und f\u00fcr maschinelles Lernen oder Zeitreihenanalysen besser nutzbar gemacht werden. Allerdings k\u00f6nnten echte, aber seltene Ereignisse (z. B. ein extremer Temperaturanstieg) ebenfalls entfernt werden. Daher sollte der Schwellenwert f\u00fcr den z-Score mit Bedacht gew\u00e4hlt und der Kontext der Daten ber\u00fccksichtigt werden.</p>"},{"location":"data-engineering/pivot_melt/","title":"Pivot und Melt in Pandas","text":"<p>Daten k\u00f6nnen in zwei grundlegenden Formaten vorliegen:</p> <ul> <li>Langes Format: Jede Beobachtung hat eine eigene Zeile. Hat es zu einem bestimmten Zeitpunkt mehrere Sensormesswerte, so stehen diese in jeweils einer neuen Zeile.</li> <li>Breites Format: Verschiedene Sensoren sind in Spalten organisiert. Jeder Sensor hat also seine eigene Spalte. Der Zeitstempel jeder Zeile gibt es nur einmal.</li> </ul> <p>Pivot- und Melt-Operationen sind notwendig, um:</p> <ul> <li>Daten f\u00fcr verschiedene Analysen in das richtige Format zu bringen</li> <li>Zeitreihendaten f\u00fcr Visualisierungen zu strukturieren</li> <li>Pivot-Tabellen f\u00fcr Berichte zu erstellen</li> <li>Daten f\u00fcr Machine Learning Modelle zu formatieren</li> <li>Zwischen verschiedenen Datenformaten zu konvertieren</li> </ul>"},{"location":"data-engineering/pivot_melt/#wann-eignet-sich-welches-format","title":"Wann eignet sich welches Format?","text":"<p>Das breite Datenformat ist f\u00fcr die meisten Anwendungsf\u00e4lle gut geeignet. Das breite Datenformat eignet sich aber besonders f\u00fcr Machine Learrning Tasks.</p> <p>Das lange Datenformat eignet sich vor allem dann, wenn es sich um Event-Daten handelt, wie zum Beispiel ein Lichtschalter. Da m\u00fcssen dann nicht z.B. alle 5 Minuten eine neue Zeile erstellt werden, sondern nur wenn sich der Wert ver\u00e4ndert. Folgendes Beispiel soll dies veranschaulichen:</p> <p>Langes Datenformat <pre><code>                                    state\ntimestamp           sensor               \n2025-04-28 10:00:00 Lichtschalter1      1\n2025-04-28 11:00:00 Lichtschalter1      0\n2025-04-28 17:00:00 Lichtschalter2      1\n2025-04-28 23:00:00 Lichtschalter2      0\n</code></pre></p> <p>Breites Datenformat <pre><code>timestamp            Lichtschalter1  Lichtschalter2\n2025-04-28 00:00:00             0.0             0.0\n2025-04-28 01:00:00             0.0             0.0\n2025-04-28 02:00:00             0.0             0.0\n2025-04-28 03:00:00             0.0             0.0\n2025-04-28 04:00:00             0.0             0.0\n2025-04-28 05:00:00             0.0             0.0\n2025-04-28 06:00:00             0.0             0.0\n2025-04-28 07:00:00             0.0             0.0\n2025-04-28 08:00:00             0.0             0.0\n2025-04-28 09:00:00             0.0             0.0\n2025-04-28 10:00:00             1.0             0.0\n2025-04-28 11:00:00             0.0             0.0\n2025-04-28 12:00:00             0.0             0.0\n2025-04-28 13:00:00             0.0             0.0\n2025-04-28 14:00:00             0.0             0.0\n2025-04-28 15:00:00             0.0             0.0\n2025-04-28 16:00:00             0.0             0.0\n2025-04-28 17:00:00             0.0             1.0\n2025-04-28 18:00:00             0.0             1.0\n2025-04-28 19:00:00             0.0             1.0\n2025-04-28 20:00:00             0.0             1.0\n2025-04-28 21:00:00             0.0             1.0\n2025-04-28 22:00:00             0.0             1.0\n2025-04-28 23:00:00             0.0             0.0\n2025-04-29 00:00:00             0.0             0.0\n</code></pre></p>"},{"location":"data-engineering/pivot_melt/#pivot-operationen","title":"Pivot-Operationen","text":"<p>Die <code>pivot()</code>-Funktion erm\u00f6glicht es, Daten von einem langen in ein breites Format zu transformieren. Bei der <code>pivot()</code>-Operation muss angegeben werden, welche Spalte den Index bildet (meistens der Zeitstempel). Welche Spalten den Namen der Messreihe beinhalten und welche Spalte die effektiven Werte beinhaltet.</p> <pre><code>import pandas as pd\n\n# Erstellen eines Beispiel-DataFrames mit Sensordaten\ndf = pd.DataFrame({\n    'Zeitstempel': pd.date_range('2025-02-01', periods=4, freq='h'),\n    'Sensor': ['Sensor1', 'Sensor2', 'Sensor1', 'Sensor2'],\n    'Temperatur': [22.5, 23.0, 24.0, 21.5]\n})\n\n# Pivot-Operation\ndf_pivot = df.pivot(\n    index='Zeitstempel',\n    columns='Sensor',\n    values='Temperatur'\n)\n\nprint(\"Pivot-Ergebnis (breites Datenformat):\")\nprint(df_pivot)\n</code></pre> <p>Asugabe: <pre><code>Pivot-Ergebnis (breites Datenformat):\nSensor               Sensor1  Sensor2\nZeitstempel                          \n2025-02-01 00:00:00     22.5      NaN\n2025-02-01 01:00:00      NaN     23.0\n2025-02-01 02:00:00     24.0      NaN\n2025-02-01 03:00:00      NaN     21.5\n</code></pre></p>"},{"location":"data-engineering/pivot_melt/#melt-operationen","title":"Melt-Operationen","text":""},{"location":"data-engineering/pivot_melt/#grundlegendes-melt","title":"Grundlegendes Melt","text":"<p>Die <code>melt()</code>-Funktion transformiert Daten von einem breiten in ein langes Format. Hier m\u00fcssen mit <code>value_vars</code> Die Spalten identifiziert werden, welche eine Messreihe darstellen. Des Weiteren muss ein neuer Name f\u00fcr die Spalte angegeben werden, die die Namen der Messreihen beinhaltet (<code>var_name</code>). Sowie f\u00fcr die Spalte, die den tats\u00e4chlichen Wert beinhaltet (<code>value_name</code>). </p> <pre><code># Erstellen eines breiten DataFrames\ndf_wide = pd.DataFrame({\n    'Zeitstempel': pd.date_range('2025-02-01', periods=2, freq='h'),\n    'Sensor1_Temperatur': [22.5, 23.0],\n    'Sensor2_Temperatur': [23.0, 24.0],\n    'Sensor1_CO2': [450, 460],\n    'Sensor2_CO2': [500, 510]\n})\n\n# Melt-Operation\ndf_melt = pd.melt(\n    df_wide,\n    id_vars=['Zeitstempel'],\n    value_vars=['Sensor1_Temperatur', 'Sensor2_Temperatur', 'Sensor1_CO2', 'Sensor2_CO2'],\n    var_name='Sensor_Messwert',\n    value_name='Wert'\n)\n\nprint(\"\\nMelt-Ergebnis (langes Datenformat):\")\nprint(df_melt)\n</code></pre> <p>Asugabe: <pre><code>Melt-Ergebnis (langes Datenformat):\n          Zeitstempel     Sensor_Messwert   Wert\n0 2025-02-01 00:00:00  Sensor1_Temperatur   22.5\n1 2025-02-01 01:00:00  Sensor1_Temperatur   23.0\n2 2025-02-01 00:00:00  Sensor2_Temperatur   23.0\n3 2025-02-01 01:00:00  Sensor2_Temperatur   24.0\n4 2025-02-01 00:00:00         Sensor1_CO2  450.0\n5 2025-02-01 01:00:00         Sensor1_CO2  460.0\n6 2025-02-01 00:00:00         Sensor2_CO2  500.0\n7 2025-02-01 01:00:00         Sensor2_CO2  510.0\n</code></pre></p>"},{"location":"data-engineering/pivot_melt/#zusammenfassung","title":"Zusammenfassung","text":"<p>Melt vs. Pivot:</p> <ul> <li><code>melt()</code>: Breit zu lang</li> <li><code>pivot()</code>: Lang zu breit</li> </ul>"},{"location":"data-engineering/resampling/","title":"Resampling von Zeitreihen","text":""},{"location":"data-engineering/resampling/#einfuhrung-und-konzepte","title":"Einf\u00fchrung und Konzepte","text":""},{"location":"data-engineering/resampling/#was-ist-resampling","title":"Was ist Resampling?","text":"<p>Resampling ist die Transformation einer Zeitreihe von einer zeitlichen Aufl\u00f6sung zu einer anderen. Es erm\u00f6glicht die Anpassung der Datengranularit\u00e4t f\u00fcr verschiedene Analysezwecke.</p>"},{"location":"data-engineering/resampling/#downsampling-vs-upsampling","title":"Downsampling vs. Upsampling","text":"<ul> <li> <p>Downsampling: Reduzierung der Frequenz durch Zusammenf\u00fchrung von Datenpunkten   <pre><code># Beispiel: Von Stunden zu Tagen\ndf.resample('D').mean()  # Tagesdurchschnitt aus Stundenwerten\n</code></pre></p> </li> <li> <p>Upsampling: Erh\u00f6hung der Frequenz durch Generierung neuer Datenpunkte   <pre><code># Beispiel: Von Tagen zu Stunden\ndf.resample('H').ffill()  # F\u00fcllt fehlende Stunden mit letztem Tageswert\n</code></pre></p> </li> </ul>"},{"location":"data-engineering/resampling/#grundlegende-methoden-ubersicht","title":"Grundlegende Methoden-\u00dcbersicht","text":"<ul> <li><code>resample()</code>: Basismethode f\u00fcr Frequenzwechsel</li> <li><code>asfreq()</code>: Direkter Frequenzwechsel ohne Aggregation</li> <li>Aggregationsmethoden: F\u00fcr Downsampling</li> <li>Interpolationsmethoden: F\u00fcr Upsampling</li> </ul> <p>Typischer Workflow: <pre><code># 1. Resampling-Objekt erstellen\nresampler = df.resample('D')\n\n# 2. Aggregation oder Interpolation anwenden\ndaily_avg = resampler.mean()  # F\u00fcr Downsampling\ndaily_filled = resampler.ffill()  # F\u00fcr Upsampling\n</code></pre></p>"},{"location":"data-engineering/resampling/#resample-die-basis-methode","title":"<code>resample()</code> - Die Basis-Methode","text":""},{"location":"data-engineering/resampling/#grundlegende-funktionsweise","title":"Grundlegende Funktionsweise","text":"<p><code>resample()</code> erstellt ein Resampling-Objekt, dem weitere Operationen folgen m\u00fcssen. Es definiert:</p> <ul> <li>Die Zielfrequenz</li> <li>Die Gruppierung der Datenpunkte</li> <li>Die Basis f\u00fcr nachfolgende Aggregation oder Interpolation</li> </ul> <p>Wichtig: <code>resample()</code> f\u00fchrt noch keine Transformation durch, sondern bereitet sie nur vor!</p> <pre><code># Nur Gruppierung, keine Daten\ngroups = df.resample('W')\n\n# Daten werden erst bei Anwendung einer Aggregation sichtbar\nweekly_mean = groups.mean()\n</code></pre>"},{"location":"data-engineering/resampling/#frequenznotation-und-parameter","title":"Frequenznotation und Parameter","text":"<p>Hauptfrequenzen:   - <code>'h'</code> \u2013 st\u00fcndlich   - <code>'min'</code> \u2013 min\u00fctlich   - <code>'s'</code> \u2013 sekundlich   - <code>'D'</code> \u2013 t\u00e4glich   - <code>'B'</code> \u2013 Werktage   - <code>'W'</code> \u2013 w\u00f6chentlich   - <code>'ME'</code> \u2013 monatlich   - <code>'QE'</code> \u2013 viertelj\u00e4hrlich   - <code>'YE'</code> \u2013 j\u00e4hrlich</p> <p>Zus\u00e4tzliche Parameter:</p> <ul> <li> <p>Zusammengesetzte Frequenzen: <code>'2h'</code>, <code>'5min'</code>, <code>'3W'</code>, etc.   <pre><code># Alle 2 Stunden\ndf.resample('2h').mean()\n\n# Alle 15 Minuten\ndf.resample('15min').mean()\n\n# Alle 6 Monate\ndf.resample('6ME').sum()\n</code></pre></p> </li> <li> <p>Ausgangsbasis: <pre><code># Monat mit Endwert als repr\u00e4sentativ\ndf.resample('ME', label='right').mean()\n</code></pre></p> </li> </ul>"},{"location":"data-engineering/resampling/#warum-resample-allein-nicht-reicht","title":"Warum resample() allein nicht reicht","text":"<p>Das Resample-Objekt enth\u00e4lt keine Daten, sondern definiert nur die Gruppierung. Es muss eine nachfolgende Operation angewendet werden:</p> <ul> <li>Aggregation f\u00fcr Downsampling</li> <li>Interpolation oder Fill f\u00fcr Upsampling</li> </ul> <p>H\u00e4ufiger Fehler: <pre><code># FALSCH: Ergebnis hat keine Werte\nresult = df.resample('D')  \n\n# RICHTIG: Aggregation oder F\u00fcllung notwendig\nresult = df.resample('D').mean()\n</code></pre></p>"},{"location":"data-engineering/resampling/#downsampling-von-hoch-zu-niedrig","title":"Downsampling: Von hoch zu niedrig","text":""},{"location":"data-engineering/resampling/#konzept","title":"Konzept","text":"<p>Downsampling kombiniert mehrere Datenpunkte zu einem, basierend auf der gew\u00e4hlten Aggregationsmethode. Die Wahl der Methode bestimmt, wie die Information erhalten bleibt.</p> <p>Beispiel-Szenario: <pre><code># Temperaturmessungen im Minutentakt zu Stundenwerten\ntemp_hourly = temp_minute.resample('H').mean()\n\n# Verkaufstransaktionen zu Tagesums\u00e4tzen\ndaily_sales = transactions.resample('D').sum()\n</code></pre></p>"},{"location":"data-engineering/resampling/#aggregationsmethoden-im-detail","title":"Aggregationsmethoden im Detail","text":""},{"location":"data-engineering/resampling/#statistische-aggregationen","title":"Statistische Aggregationen","text":"<ul> <li> <p><code>mean()</code> - Arithmetischer Mittelwert</p> <ul> <li>Berechnet den Durchschnitt aller Werte</li> <li>Gl\u00e4ttet Ausreisser und Variationen</li> </ul> <pre><code># Durchschnittliche Temperatur pro Tag\ndaily_avg_temp = hourly_temp.resample('D').mean()\n</code></pre> </li> <li> <p><code>sum()</code> - Summation</p> <ul> <li>Addiert alle Werte innerhalb des Zeitfensters</li> <li>Erh\u00e4lt die Gesamtmenge</li> </ul> <pre><code># Gesamter Regenfall pro Monat\nmonthly_rainfall = daily_rainfall.resample('ME').sum()\n</code></pre> </li> <li> <p><code>max()</code> - Maximum</p> <ul> <li>Beh\u00e4lt den h\u00f6chsten Wert</li> <li>Wichtig f\u00fcr Spitzenwerte</li> </ul> <pre><code># Maximale Temperatur pro Monat\nmonthly_max_temp = daily_temp.resample('ME').max()\n</code></pre> </li> <li> <p><code>min()</code> - Minimum</p> <ul> <li>Beh\u00e4lt den niedrigsten Wert</li> <li>Kritisch f\u00fcr Grenzwert\u00fcberwachung</li> </ul> <pre><code># Niedrigste Luftfeuchtigkeit pro Monat\nmonthly_min_humidity = hourly_humidity.resample('ME').min()\n</code></pre> </li> <li> <p><code>count()</code> - Anzahl</p> <ul> <li>Z\u00e4hlt g\u00fcltige Werte (nicht NaN)</li> <li>F\u00fcr Datenqualit\u00e4tspr\u00fcfung</li> </ul> <pre><code># Anzahl der Messungen pro Tag\ndaily_measurements = hourly_data.resample('D').count()\n</code></pre> </li> <li> <p><code>std()</code> - Standardabweichung</p> <ul> <li>Misst die Streuung innerhalb des Zeitfensters</li> <li>Indikator f\u00fcr Variabilit\u00e4t</li> </ul> <pre><code># Temperaturvariabilit\u00e4t pro Woche\nweekly_temp_std = daily_temp.resample('W').std()\n</code></pre> </li> <li> <p><code>median()</code> - Median</p> <ul> <li>Mittlerer Wert bei Sortierung</li> <li>Robust gegen Ausreisser</li> </ul> <pre><code># Medianpreis pro Monat (weniger anf\u00e4llig f\u00fcr extreme Werte)\nmonthly_median_price = daily_prices.resample('ME').median()\n</code></pre> </li> </ul>"},{"location":"data-engineering/resampling/#temporale-aggregationen","title":"Temporale Aggregationen","text":"<ul> <li> <p><code>first()</code> - Erster Wert</p> <ul> <li>Nimmt den ersten g\u00fcltigen Wert</li> <li>Repr\u00e4sentiert Zeitfensterbeginn</li> </ul> <pre><code># Er\u00f6ffnungskurs am Beginn jeder Woche\nweekly_open = daily_prices.resample('W').first()\n</code></pre> </li> <li> <p><code>last()</code> - Letzter Wert</p> <ul> <li>Nimmt den letzten g\u00fcltigen Wert</li> <li>Repr\u00e4sentiert Zeitfensterende</li> </ul> <pre><code># Schlusskurs am Ende jeder Woche\nweekly_close = daily_prices.resample('W').last()\n</code></pre> </li> </ul>"},{"location":"data-engineering/resampling/#benutzerdefinierte-aggregationen","title":"Benutzerdefinierte Aggregationen","text":"<p>Mit <code>agg()</code> f\u00fcr spaltenweise Aggregation: <pre><code># Verschiedene Aggregationen pro Spalte\ndf.resample('H').agg({\n    'temperatur': 'mean',      # Durchschnitt f\u00fcr Temperatur\n    'luftfeuchtigkeit': 'mean',# Durchschnitt f\u00fcr Luftfeuchtigkeit\n    'niederschlag': 'sum',     # Summe f\u00fcr Niederschlag\n    'windgeschwindigkeit': ['mean', 'max'] # Mehrfache Aggregationen\n})\n\n# Mehrfache Aggregationen f\u00fcr alle Spalten\ndf.resample('D').agg(['mean', 'std', 'count'])\n</code></pre></p>"},{"location":"data-engineering/resampling/#upsampling-von-niedrig-zu-hoch","title":"Upsampling: Von niedrig zu hoch","text":""},{"location":"data-engineering/resampling/#konzept_1","title":"Konzept","text":"<p>Upsampling generiert neue Zeitindizes zwischen existierenden Datenpunkten. Die Werte m\u00fcssen explizit gef\u00fcllt oder interpoliert werden.</p> <p>Wichtig: Upsampling \"erfindet\" keine neuen Daten, sondern sch\u00e4tzt sie basierend auf vorhandenen Informationen!</p>"},{"location":"data-engineering/resampling/#asfreq-methode","title":"<code>asfreq()</code> - Methode","text":"<p>Grundfunktion:</p> <ul> <li>Erzeugt neue Zeitindizes mit der Zielfrequenz</li> <li>F\u00fcllt Werte initial mit NaN</li> <li>Keine automatische Datengenerierung</li> </ul> <p>Anwendung: <pre><code># Grundlegendes Upsampling (alle neuen Werte = NaN)\nhourly_empty = daily_data.resample('H').asfreq()\n\n# Mit Forward Fill - letzte Werte werden vorw\u00e4rts gef\u00fcllt\nhourly_filled = daily_data.resample('H').asfreq(method='ffill')\n\n# Downsampling mit asfreq (nimmt letzten Wert)\ndaily_end = hourly_data.resample('D').asfreq(how='end')\n</code></pre></p>"},{"location":"data-engineering/resampling/#fill-methoden","title":"Fill-Methoden","text":""},{"location":"data-engineering/resampling/#forwardbackward-fill","title":"Forward/Backward Fill","text":"<ul> <li> <p><code>ffill()</code> - Forward Fill</p> <ul> <li>Propagiert letzten bekannten Wert vorw\u00e4rts</li> <li>Bis zum n\u00e4chsten g\u00fcltigen Datenpunkt</li> </ul> <pre><code># Beispiel: Preise \"halten\" zwischen Handelstagen\ndaily_prices = weekly_prices.resample('D').ffill()\n</code></pre> </li> <li> <p><code>bfill()</code> - Backward Fill</p> <ul> <li>Propagiert n\u00e4chsten bekannten Wert r\u00fcckw\u00e4rts</li> <li>F\u00fcr F\u00fcllungen in Richtung Vergangenheit</li> </ul> <pre><code># Beispiel: Vorausschau auf n\u00e4chsten bekannten Wert\nfilled_backwards = monthly_data.resample('D').bfill()\n</code></pre> </li> </ul>"},{"location":"data-engineering/resampling/#spezifische-fullwerte","title":"Spezifische F\u00fcllwerte","text":"<p><code>fillna()</code> mit Konstanten: <pre><code># Alle neuen Werte mit 0 f\u00fcllen\nhourly_data = daily_data.resample('H').asfreq().fillna(0)\n\n# Verschiedene F\u00fcllwerte pro Spalte\nhourly_data = daily_data.resample('H').asfreq()\nhourly_data['temperature'].fillna(20)  # Standardtemperatur\nhourly_data['humidity'].fillna(method='ffill')  # Forward Fill f\u00fcr Feuchtigkeit\n</code></pre></p>"},{"location":"data-engineering/resampling/#interpolationsmethoden","title":"Interpolationsmethoden","text":""},{"location":"data-engineering/resampling/#mathematische-interpolationstypen","title":"Mathematische Interpolationstypen","text":"<ul> <li> <p>Lineare Interpolation (<code>linear</code>)</p> <ul> <li>Gerade Linien zwischen Punkten</li> <li>Standard f\u00fcr stetige Daten</li> </ul> <pre><code># Gleichm\u00e4ssige Verteilung zwischen Messpunkten\nhourly_linear = daily_temp.resample('H').interpolate(method='linear')\n</code></pre> </li> <li> <p>Zeitbasierte Interpolation (<code>time</code>)</p> <ul> <li>Gewichtet nach zeitlichen Abst\u00e4nden</li> <li>F\u00fcr ungleichm\u00e4ssige Zeitintervalle</li> </ul> <pre><code># Ber\u00fccksichtigt tats\u00e4chliche Zeitabst\u00e4nde\ninterpolated = irregular_data.resample('H').interpolate(method='time')\n</code></pre> </li> <li> <p>N\u00e4chster Nachbar (<code>nearest</code>)</p> <ul> <li>Kein \u00dcbergang, Stufenfunktion</li> <li>F\u00fcr diskrete Werte</li> </ul> <pre><code># Diskrete Werte wie Kategorien\ncategory_hourly = daily_category.resample('H').interpolate(method='nearest')\n</code></pre> </li> <li> <p>Polynomische Interpolation (<code>polynomial</code>)</p> <ul> <li>Polynome n-ter Ordnung</li> <li>Parameter <code>order</code> bestimmt Grad</li> </ul> <pre><code># Glatte Kurven f\u00fcr nat\u00fcrliche Ph\u00e4nomene\nsmooth_curve = daily_data.resample('H').interpolate(method='polynomial', order=3)\n</code></pre> </li> </ul>"},{"location":"data-engineering/time_indexes/","title":"Arbeiten mit Time-Indexes in Pandas","text":"<p>In diesem Dokument gehen wir auf die Grundlagen der Arbeit mit Zeitstempeln und Zeitindizes in Pandas ein. Dabei behandeln wir wichtige Themen wie das Finden fehlender Daten, das Sortieren von Indizes, das Arbeiten mit Zeitstempeln (z. B. Umwandlung in <code>datetime</code>), Zeitzonen und die Zeitumstellung (Sommerzeit).</p>"},{"location":"data-engineering/time_indexes/#erstellen-eines-dataframes-mit-zeitstempeln-und-fehlenden-daten","title":"Erstellen eines DataFrames mit Zeitstempeln und fehlenden Daten","text":"<p>Zun\u00e4chst erstellen wir einen einfachen DataFrame mit Zeitstempeln, einer Temperatur-Spalte und fehlenden Werten (<code>NA</code>). Dabei sorgen wir daf\u00fcr, dass die Zeitstempel korrekt als <code>datetime</code> erkannt werden, was f\u00fcr sp\u00e4tere Zeitreihen-Operationen wichtig ist.</p> <pre><code>import pandas as pd\n\n# Erstellen des DataFrames mit Zeitstempeln und fehlenden Werten (NA)\ndata_unsorted = [22.5, 23.0, pd.NA, 24.0, 25.0]\ndates_unsorted = ['2025-02-03', '2025-02-01', '2025-02-02', '2025-02-05', '2025-02-04']\n\n# Erstellen des DataFrames ohne Index\ndf = pd.DataFrame(data_unsorted, columns=['Temperatur'])\n\n# Umwandlung der Datumsangaben in datetime und Setzen des Index\ndf['Datum'] = pd.to_datetime(dates_unsorted)\ndf = df.set_index('Datum')\n\n# Ausgabe des DataFrames\nprint(df)\n</code></pre> <p>Ausgabe: <pre><code>           Temperatur\nDatum\n2025-02-03       22.5\n2025-02-01       23.0\n2025-02-02       &lt;NA&gt;\n2025-02-05       24.0\n2025-02-04       25.0\n</code></pre></p>"},{"location":"data-engineering/time_indexes/#erklarung","title":"Erkl\u00e4rung:","text":"<ul> <li>Die Zeitstempel in der Liste <code>dates_unsorted</code> werden mit <code>pd.to_datetime()</code> in das <code>datetime</code>-Format konvertiert.</li> <li>Der DataFrame wird dann mit <code>set_index()</code> so ver\u00e4ndert, dass die <code>Datum</code>-Spalte als Index verwendet wird.</li> </ul>"},{"location":"data-engineering/time_indexes/#index-sortierung","title":"Index-Sortierung","text":"<p>Nach der Umwandlung der Zeitstempel in <code>datetime</code> m\u00f6chten wir den Index so sortieren, dass die Zeitstempel in der richtigen Reihenfolge sind. Daf\u00fcr verwenden wir <code>sort_index()</code>.</p> <pre><code># Sortieren des Indexes nach Datum\ndf = df.sort_index()\n\n# Ausgabe des sortierten DataFrames\nprint(df)\n</code></pre> <p>Ausgabe: <pre><code>           Temperatur\nDatum\n2025-02-01       23.0\n2025-02-02       &lt;NA&gt;\n2025-02-03       22.5\n2025-02-04       25.0\n2025-02-05       24.0\n</code></pre></p>"},{"location":"data-engineering/time_indexes/#fehlende-daten-finden","title":"Fehlende Daten finden","text":"<p>Fehlende Werte sind h\u00e4ufig in Zeitreihen und k\u00f6nnen durch verschiedene Faktoren wie Sensorfehler oder Datenl\u00fccken entstehen. In Pandas k\u00f6nnen Sie fehlende Werte leicht erkennen, indem Sie <code>isnull()</code> verwenden.</p>"},{"location":"data-engineering/time_indexes/#beispiel-fehlende-daten-finden","title":"Beispiel: Fehlende Daten finden","text":"<pre><code># Ausgabe der Zeilen mit fehlenden Daten\nprint(\"Zeilen mit fehlenden Daten:\")\nprint(df[df.isnull().values])\n</code></pre> <p>Ausgabe: <pre><code>Zeilen mit fehlenden Daten:\n           Temperatur\nDatum\n2025-02-02       &lt;NA&gt;\n</code></pre></p>"},{"location":"data-engineering/time_indexes/#zeitzonen-und-sommerzeit","title":"Zeitzonen und Sommerzeit","text":"<p>Es ist wichtig, sicherzustellen, dass Zeitstempel mit der richtigen Zeitzone und unter Ber\u00fccksichtigung der Sommerzeit (Daylight Saving Time) behandelt werden. Um dies zu erreichen, k\u00f6nnen Sie die <code>tz_localize()</code> und <code>tz_convert()</code> Funktionen in Pandas verwenden.</p>"},{"location":"data-engineering/time_indexes/#beispiel-umgang-mit-zeitzonen-und-sommerzeit","title":"Beispiel: Umgang mit Zeitzonen und Sommerzeit","text":"<p>In diesem Beispiel stellen wir sicher, dass die Zeitstempel korrekt lokalisiert und in eine andere Zeitzone konvertiert werden, dabei die Sommerzeit korrekt ber\u00fccksichtigt wird.</p> <pre><code># Lokalisieren des DatetimeIndex in der Zeitzone Z\u00fcrich\ndf.index = df.index.tz_localize('Europe/Zurich')\n\n# Umwandeln der Zeitzone in UTC\ndf.index = df.index.tz_convert('UTC')\n\n# Ausgabe des DataFrames mit Zeitzonenbehandlung\nprint(df)\n</code></pre> <p>Ausgabe: <pre><code>                          Temperatur\nDatum\n2025-01-31 23:00:00+00:00       23.0\n2025-02-01 23:00:00+00:00       &lt;NA&gt;\n2025-02-02 23:00:00+00:00       22.5\n2025-02-03 23:00:00+00:00       25.0\n2025-02-04 23:00:00+00:00       24.0\n</code></pre></p> <p>Erkl\u00e4rung:</p> <ul> <li>Zun\u00e4chst haben wir den <code>datetime</code>-Index in die Zeitzone <code>Europe/Zurich</code> lokalisiert.</li> <li>Anschliessend wurde der Index in die UTC-Zeitzone konvertiert.</li> </ul>"},{"location":"data-protection/data_protection/","title":"Datenschutz und DSGVO: Grundlagen und wichtige Konzepte","text":""},{"location":"data-protection/data_protection/#was-ist-datenschutz","title":"Was ist Datenschutz?","text":"<p>Datenschutz bezieht sich auf den Schutz personenbezogener Daten und die Regelung ihrer Verarbeitung. In der Europ\u00e4ischen Union wird Datenschutz haupts\u00e4chlich durch die Datenschutz-Grundverordnung (DSGVO) geregelt.</p>"},{"location":"data-protection/data_protection/#was-ist-die-dsgvo","title":"Was ist die DSGVO?","text":"<p>Die DSGVO (Datenschutz-Grundverordnung) ist die umfassende Datenschutzregelung der Europ\u00e4ischen Union. Sie trat am 25. Mai 2018 in Kraft und gilt f\u00fcr alle Unternehmen und Organisationen, die personenbezogene Daten von EU-B\u00fcrgern verarbeiten, unabh\u00e4ngig vom Standort des Unternehmens.</p> <p>Weitere Informationen finden Sie unter: https://dsgvo-gesetz.de/</p>"},{"location":"data-protection/data_protection/#grundsatze-fur-die-verarbeitung-personenbezogener-daten-dsgvo-art-5","title":"Grunds\u00e4tze f\u00fcr die Verarbeitung personenbezogener Daten (DSGVO Art. 5)","text":"<p>Die DSGVO legt sieben Kernprinzipien fest, die bei der Verarbeitung personenbezogener Daten eingehalten werden m\u00fcssen:</p> <ol> <li> <p>Rechtm\u00e4ssigkeit: Jede Datenverarbeitung ben\u00f6tigt eine klare gesetzliche Grundlage wie Einwilligung, Vertrag, etc.</p> </li> <li> <p>Transparenz: Betroffene Personen m\u00fcssen klar und verst\u00e4ndlich \u00fcber die Verarbeitung ihrer Daten informiert werden.</p> </li> <li> <p>Zweckbindung: Daten d\u00fcrfen nur f\u00fcr vorab festgelegte, eindeutige und legitime Zwecke erhoben werden.</p> </li> <li> <p>Datenminimierung: Es sollten nur die f\u00fcr den angegebenen Zweck notwendigen Daten erhoben werden.</p> </li> <li> <p>Richtigkeit: Die verarbeiteten Daten m\u00fcssen korrekt und aktuell sein.</p> </li> <li> <p>Speicherbegrenzung: Personenbezogene Daten m\u00fcssen gel\u00f6scht werden, sobald der Zweck ihrer Verarbeitung erf\u00fcllt ist.</p> </li> <li> <p>Integrit\u00e4t: Daten m\u00fcssen vor unbefugtem Zugriff, Verlust oder Besch\u00e4digung gesch\u00fctzt werden.</p> </li> </ol>"},{"location":"data-protection/data_protection/#recht-auf-loschung-right-to-be-forgotten-dsgvo-art-7","title":"Recht auf L\u00f6schung (\"Right to be forgotten\") (DSGVO Art. 7)","text":"<p>Die DSGVO gibt Personen das Recht, die L\u00f6schung ihrer Daten zu verlangen. Unternehmen sind in folgenden Situationen zur L\u00f6schung verpflichtet:</p> <ul> <li>Wenn der urspr\u00fcngliche Zweck der Datenerhebung erf\u00fcllt ist</li> <li>Wenn die betroffene Person ihre Einwilligung widerruft</li> <li>Bei unrechtm\u00e4ssiger Verarbeitung der Daten</li> </ul>"},{"location":"data-protection/data_protection/#besonders-schutzenswerte-daten-dsgvo-art-17","title":"Besonders sch\u00fctzenswerte Daten (DSGVO Art. 17)","text":"<p>Bestimmte Datenkategorien werden als besonders sch\u00fctzenswert eingestuft und unterliegen strengeren Schutzmassnahmen:</p>"},{"location":"data-protection/data_protection/#kategorien-besonders-schutzenswerter-daten","title":"Kategorien besonders sch\u00fctzenswerter Daten:","text":"<ul> <li>Rassische und ethnische Herkunft</li> <li>Politische und religi\u00f6se Meinungen</li> <li>Gesundheitsdaten, genetische und biometrische Daten</li> <li>Daten zum Sexualleben oder zur sexuellen Orientierung</li> </ul>"},{"location":"data-protection/data_protection/#ausnahmen-fur-die-verarbeitung-besonders-schutzenswerter-daten","title":"Ausnahmen f\u00fcr die Verarbeitung besonders sch\u00fctzenswerter Daten:","text":"<ul> <li>Ausdr\u00fcckliche Einwilligung der betroffenen Person</li> <li>Lebenswichtige Interessen: Wenn die Verarbeitung zum Schutz lebenswichtiger Interessen notwendig ist, oder f\u00fcr Arbeitsrecht oder Rechtsanspr\u00fcche</li> <li>\u00d6ffentliches Interesse: Bei Themen der \u00f6ffentlichen Gesundheit, wissenschaftlicher Forschung oder Archivierung</li> </ul>"},{"location":"data-protection/data_protection/#datenschutz-in-der-praxis","title":"Datenschutz in der Praxis","text":"<p>Die Umsetzung des Datenschutzes in technischen Anwendungsbereichen erfordert einen ganzheitlichen Ansatz, der folgende Kernaspekte ber\u00fccksichtigt:</p>"},{"location":"data-protection/data_protection/#technische-und-organisatorische-massnahmen","title":"Technische und organisatorische Massnahmen","text":"<ul> <li>Privacy by Design: Datenschutz muss von Anfang an in alle technischen Systeme integriert werden, ob in der Geb\u00e4udeautomation, Energietechnik oder Data Science</li> <li>Datenminimierung: In Smart Buildings nur notwendige Sensordaten erfassen, bei Smart Metern aggregierte statt individuelle Messungen bevorzugen, in der KI auf synthetische Datens\u00e4tze zur\u00fcckgreifen</li> <li>Verschl\u00fcsselung und Sicherheit: Durchg\u00e4ngige Verschl\u00fcsselung bei der Daten\u00fcbertragung in IoT-Ger\u00e4ten, Zutrittssystemen und Energiemanagementsystemen</li> <li>Zugriffsmanagement: Rollenbasierte Zugriffsbeschr\u00e4nkungen f\u00fcr sensible Daten in allen technischen Bereichen implementieren</li> </ul>"},{"location":"data-protection/data_protection/#transparenz-und-einwilligung","title":"Transparenz und Einwilligung","text":"<ul> <li>Nutzerinformation: Geb\u00e4udenutzer \u00fcber eingesetzte Sensorik informieren, Energieverbraucher \u00fcber Datennutzung aufkl\u00e4ren, Betroffene \u00fcber KI-Entscheidungen informieren</li> <li>Einwilligungsmanagement: Granulare Einwilligungsoptionen f\u00fcr Smart-Home-Funktionen, detaillierte Energieverbrauchsanalysen und personalisierte Dienste anbieten</li> <li>Nachvollziehbarkeit: Transparente Verfahren in der Geb\u00e4udeautomation, nachvollziehbare Algorithmen in Energiemanagementsystemen und erkl\u00e4rbare KI-Entscheidungen sicherstellen</li> </ul>"},{"location":"data-protection/data_protection/#datenschutzmanagement-und-compliance","title":"Datenschutzmanagement und Compliance","text":"<ul> <li>L\u00f6schkonzepte: Automatisierte L\u00f6schung tempor\u00e4rer \u00dcberwachungsdaten, Verbrauchsdaten und Trainingsdaten nach Zweckerf\u00fcllung</li> <li>Regelm\u00e4ssige Audits: Periodische \u00dcberpr\u00fcfung aller datenschutzrelevanten Systeme in der technischen Geb\u00e4udeausr\u00fcstung, Energietechnik und datengest\u00fctzten Umwelt\u00fcberwachung</li> <li>Datenschutz-Folgenabsch\u00e4tzungen: Besonders bei vernetzten Systemen wie Smart Buildings, intelligenten Stromnetzen und KI-gest\u00fctztem Umweltmonitoring durchf\u00fchren</li> <li>Pseudonymisierung: Trennung von Identifikations- und technischen Daten in allen Anwendungsbereichen, insbesondere bei der Verkn\u00fcpfung von Geb\u00e4ude-, Energie- und Umweltdaten</li> </ul>"},{"location":"machine-learning/basics/","title":"Grundlagen des Machine Learning","text":""},{"location":"machine-learning/basics/#was-ist-kunstliche-intelligenz-ai","title":"Was ist K\u00fcnstliche Intelligenz (AI)?","text":"<p>K\u00fcnstliche Intelligenz bezeichnet Computersysteme, die Aufgaben ausf\u00fchren, die normalerweise menschliche Intelligenz erfordern, wie Sprachverst\u00e4ndnis, Entscheidungsfindung und Probleml\u00f6sung.</p>"},{"location":"machine-learning/basics/#was-ist-machine-learning","title":"Was ist Machine Learning?","text":"<p>Machine Learning ist ein Teilbereich der AI, bei dem Computersysteme aus Daten lernen und sich verbessern, ohne explizit programmiert zu werden. Algorithmen analysieren Daten und leiten daraus Muster oder Entscheidungen ab.</p>"},{"location":"machine-learning/basics/#was-ist-ein-modell","title":"Was ist ein Modell?","text":"<p>Ein Modell ist eine mathematische Repr\u00e4sentation eines Prozesses, das durch Trainingsdaten erstellt wird. Es enth\u00e4lt gelernten Parameter, um Vorhersagen f\u00fcr neue Daten zu treffen. Diese Parameter werden w\u00e4hrend des Trainingsprozesses durch Optimierung angepasst. Der Fehler (der Unterschied zwischen den Vorhersagen des Modells und den tats\u00e4chlichen Ergebnissen) wird berechnet und verwendet, um das Modell zu verbessern.</p>"},{"location":"machine-learning/basics/#input-features-und-output","title":"Input Features und Output","text":"<p>Input Features sind die Merkmale, die das Modell nutzt, um Vorhersagen zu treffen, wie z. B. Wohnfl\u00e4che (m\u00b2) und Anzahl der Zimmer beim Vorhersagen des Hauspreises. Diese Features k\u00f6nnen numerisch oder kategorisch sein.</p> <p>Output ist das Ergebnis des Modells, das eine Vorhersage basierend auf den Input Features darstellt, wie z. B. der Preis eines Hauses oder die Kategorie einer E-Mail (z. B. \u201eSpam\u201c oder \u201eNicht-Spam\u201c).</p>"},{"location":"machine-learning/basics/#was-ist-eine-regression","title":"Was ist eine Regression?","text":"<p>Regression ist eine Methode zur Vorhersage kontinuierlicher Werte, basierend auf dem Zusammenhang zwischen Eingabe- und Zielvariablen.</p>"},{"location":"machine-learning/basics/#beispiel-fur-regression","title":"Beispiel f\u00fcr Regression","text":"<p>Angenommen, wir wollen den Preis eines Hauses vorhersagen. Wir nutzen Wohnfl\u00e4che (m\u00b2) und Anzahl der Zimmer als Input Features. Das Modell gibt dann einen kontinuierlichen Wert \u2013 den Preis des Hauses \u2013 als Output zur\u00fcck.</p>"},{"location":"machine-learning/basics/#residuen","title":"Residuen","text":"<p>Residuen (oder Fehler) sind die Differenzen zwischen den tats\u00e4chlichen und den vorhergesagten Werten eines Modells. F\u00fcr ein einzelnes Beispiel berechnet man das Residuum, indem man den tats\u00e4chlichen Wert vom vorhergesagten Wert subtrahiert:</p> \\[ \\text{Residuum} = \\text{tats\u00e4chlicher Wert} - \\text{vorhergesagter Wert} \\] <p>Residuen geben an, wie gut oder schlecht das Modell bei der Vorhersage von Ergebnissen ist. Wenn die Residuen nahe null sind, bedeutet dies, dass das Modell gute Vorhersagen trifft. Grosse Residuen deuten darauf hin, dass das Modell Fehler gemacht hat, die verbessert werden sollten.</p>"},{"location":"machine-learning/basics/#metriken","title":"Metriken","text":"<p>Metriken sind Masse, die verwendet werden, um die Leistung eines Modells zu bewerten, insbesondere im Hinblick auf die Fehler, die es macht. Zwei der h\u00e4ufigsten Metriken im Bereich der Regression sind Mean Absolute Error (MAE) und Mean Squared Error (MSE).</p>"},{"location":"machine-learning/basics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Der Mean Absolute Error (MAE) misst den durchschnittlichen absoluten Fehler zwischen den tats\u00e4chlichen und den vorhergesagten Werten. Es wird berechnet, indem die absoluten Werte der Residuen gemittelt werden:</p> \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\text{tats\u00e4chlicher Wert}_i - \\text{vorhergesagter Wert}_i| \\] <p>Vorteil: MAE ist leicht zu verstehen, da es einfach den Durchschnitt der absoluten Fehler misst. Ein niedriger MAE-Wert zeigt, dass das Modell im Durchschnitt genau ist.</p>"},{"location":"machine-learning/basics/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Der Mean Squared Error (MSE) misst den durchschnittlichen quadratischen Fehler zwischen den tats\u00e4chlichen und den vorhergesagten Werten. Es wird berechnet, indem die quadratischen Residuen gemittelt werden:</p> \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{tats\u00e4chlicher Wert}_i - \\text{vorhergesagter Wert}_i)^2 \\] <p>Vorteil: MSE bestraft gr\u00f6ssere Fehler st\u00e4rker, da gr\u00f6ssere Fehler quadratisch wachsen. Das bedeutet, dass das Modell dazu tendiert, Fehler zu minimieren, die besonders gross sind. MSE wird h\u00e4ufig bevorzugt, wenn grosse Fehler zu vermeiden sind.</p>"},{"location":"machine-learning/linear_regression/","title":"Lineare Regression","text":""},{"location":"machine-learning/linear_regression/#was-ist-lineare-regression","title":"Was ist Lineare Regression?","text":"<p>Die lineare Regression ist eine Methode, um den Zusammenhang zwischen einer abh\u00e4ngigen Variablen \\(y\\) und einer unabh\u00e4ngigen Variablen \\(x\\) zu modellieren. Das Ziel der linearen Regression ist es, eine gerade Linie zu finden, die die Datenpunkte bestm\u00f6glich beschreibt.</p> <p>Die grundlegende Gleichung f\u00fcr eine Gerade in der linearen Regression lautet:</p> \\[ y = mx + b \\] <ul> <li>\\(y\\) ist die abh\u00e4ngige Variable (das, was wir vorhersagen wollen).</li> <li>\\(x\\) ist die unabh\u00e4ngige Variable (das Feature oder der Input).</li> <li>\\(m\\) ist die Steigung der Linie (wie stark \\(y\\) sich ver\u00e4ndert, wenn \\(x\\) sich \u00e4ndert).</li> <li>\\(b\\) ist der y-Achsenabschnitt (der Wert von \\(y\\), wenn \\(x = 0\\)).</li> </ul>"},{"location":"machine-learning/linear_regression/#wie-funktioniert-lineare-regression","title":"Wie funktioniert Lineare Regression?","text":"<p>In der linearen Regression geht es darum, die Werte f\u00fcr \\(m\\) (Steigung) und \\(b\\) (y-Achsenabschnitt) so zu finden, dass die Gerade die Datenpunkte m\u00f6glichst gut beschreibt. Dies geschieht durch die Optimierung des MSE (Mean Squared Error), bei dem der Fehler zwischen den tats\u00e4chlichen und den vorhergesagten Werten minimiert wird.</p>"},{"location":"machine-learning/linear_regression/#vorhersage-mit-der-linearen-regression","title":"Vorhersage mit der Linearen Regression","text":"<p>Nachdem die Parameter \\(m\\) und \\(b\\) durch das Training des Modells bestimmt wurden, k\u00f6nnen wir mit der Gleichung \\(y = mx + b\\) Vorhersagen f\u00fcr neue Werte von \\(x\\) treffen.</p>"},{"location":"machine-learning/linear_regression/#beispiel-fur-lineare-regression","title":"Beispiel f\u00fcr Lineare Regression","text":"<p>Stellen wir uns vor, wir m\u00f6chten den Preis eines Hauses basierend auf der Wohnfl\u00e4che vorhersagen. Angenommen, wir haben die folgenden Daten:</p> Wohnfl\u00e4che (m\u00b2) Preis (\u20ac) 50 150,000 75 200,000 100 250,000 125 300,000 <p>Die lineare Regression w\u00fcrde eine Gerade \\(y = mx + b\\) berechnen, bei der:</p> <ul> <li>\\(y\\) der Preis des Hauses ist.</li> <li>\\(x\\) die Wohnfl\u00e4che des Hauses ist.</li> <li>\\(m\\) die Steigung der Linie beschreibt (wie der Preis mit der Fl\u00e4che w\u00e4chst).</li> <li>\\(b\\) der y-Achsenabschnitt ist (der Preis, wenn die Fl\u00e4che 0 ist, was in der Praxis wenig Bedeutung hat, aber ein mathematischer Wert ist).</li> </ul>"},{"location":"machine-learning/linear_regression/#implementierung-der-linearen-regression","title":"Implementierung der Linearen Regression","text":"<p>Um eine lineare Regression in Python durchzuf\u00fchren, k\u00f6nnen wir die Bibliothek <code>scikit-learn</code> verwenden. Hier ist ein einfaches Beispiel:</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nX = data[['x']] # Bei mehreren Features: X = data[['x1', 'x2', ...]]\ny = data['y']\nmodel.fit(X, y) \n</code></pre>"},{"location":"machine-learning/linear_regression/#vorhersage-mit-dem-trainierten-modell","title":"Vorhersage mit dem trainierten Modell","text":"<p>In diesem Beispiel wird ein <code>LinearRegression</code>-Modell erstellt und mit den Daten <code>X</code> (unabh\u00e4ngige Variable) und <code>y</code> (abh\u00e4ngige Variable) trainiert. Nach dem Training k\u00f6nnen wir Vorhersagen treffen, indem wir die Methode <code>predict()</code> verwenden:</p> <pre><code>predictions = model.predict(X) # Vorhersagen f\u00fcr die Trainingsdaten\n</code></pre> <p>Um eine Vorhersage f\u00fcr einen neuen Wert zu treffen, k\u00f6nnen wir die Methode <code>predict()</code> mit dem neuen Wert aufrufen:</p> <pre><code># Vorhersage f\u00fcr eine Fl\u00e4che von 150 m\u00b2\nprint(model.predict([[150]]))\n</code></pre> <p>Oder f\u00fcr mehrere Werte:</p> <pre><code># Vorhersage f\u00fcr mehrere Fl\u00e4chen\nprint(model.predict([[150], [200], [250]]))\n</code></pre>"},{"location":"machine-learning/linear_regression/#achsenabschnitt-und-steigung","title":"Achsenabschnitt und Steigung","text":"<p>Den Achsenabschnitt und die Steigung k\u00f6nnen wir mit den Attributen <code>intercept_</code> und <code>coef_</code> des Modells abrufen:</p> <pre><code>intercept = model.intercept_ # y-Achsenabschnitt\nslope = model.coef_ # Steigung\nprint(f\"y-Achsenabschnitt: {intercept}, Steigung: {slope}\")\n</code></pre> <p>Die mathematische Gleichung der linearen Regression lautet dann: $$ y = \\text{slope} \\cdot x + \\text{intercept} $$</p>"},{"location":"machine-learning/linear_regression/#visualisierung-der-ergebnisse","title":"Visualisierung der Ergebnisse","text":"<p>Um die Ergebnisse der linearen Regression zu visualisieren, k\u00f6nnen wir die Bibliothek <code>matplotlib</code> verwenden. Hier ist ein einfaches Beispiel:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Scatterplot der Datenpunkte\nplt.scatter(data['x'], data['y'], color='blue')\n\n# Vorhersage der y-Werte f\u00fcr die x-Werte (Unser Modell)\nplt.plot(data['x'], model.predict(X), color='red') \n\nplt.show()\n</code></pre>"},{"location":"machine-learning/model_evaluation/","title":"Modellevaluation","text":""},{"location":"machine-learning/model_evaluation/#was-ist-ein-train-test-split","title":"Was ist ein Train-Test Split?","text":"<p>Der Train-Test Split ist eine fundamentale Methode im Machine Learning, bei der der Datensatz in zwei Teile aufgeteilt wird:</p> <ul> <li>Trainingsdaten: Werden verwendet, um das Modell zu trainieren</li> <li>Testdaten: Werden verwendet, um die Leistung des Modells zu bewerten</li> </ul> <p>Diese Aufteilung ist essentiell, um zu beurteilen, wie gut ein Modell auf neue, unbekannte Daten reagiert.</p>"},{"location":"machine-learning/model_evaluation/#warum-ist-train-test-split-wichtig","title":"Warum ist Train-Test Split wichtig?","text":"<p>Ohne Train-Test Split w\u00fcrden wir das Modell auf denselben Daten trainieren und bewerten. Das f\u00fchrt zu unrealistisch guten Ergebnissen, da das Modell die Trainingsdaten bereits \"kennt\". Mit einem Train-Test Split k\u00f6nnen wir Overfitting erkennen und die tats\u00e4chliche Leistung des Modells einsch\u00e4tzen.</p>"},{"location":"machine-learning/model_evaluation/#overfitting-und-underfitting-verstehen","title":"Overfitting und Underfitting verstehen","text":"<p>https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76</p>"},{"location":"machine-learning/model_evaluation/#was-ist-overfitting","title":"Was ist Overfitting?","text":"<p>Overfitting tritt auf, wenn ein Modell die Trainingsdaten zu genau lernt, einschliesslich des Rauschens. Das Modell funktioniert sehr gut auf den Trainingsdaten, aber schlecht auf neuen Daten.</p> <p>Anzeichen f\u00fcr Overfitting:</p> <ul> <li>Sehr niedriger Training MSE, aber hoher Test MSE</li> <li>Grosse Differenz zwischen Training- und Test-Performance</li> </ul>"},{"location":"machine-learning/model_evaluation/#was-ist-underfitting","title":"Was ist Underfitting?","text":"<p>Underfitting tritt auf, wenn ein Modell zu einfach ist und die zugrunde liegenden Muster in den Daten nicht erfassen kann.</p> <p>Anzeichen f\u00fcr Underfitting:</p> <ul> <li>Hoher Training MSE und hoher Test MSE</li> <li>Beide Werte sind \u00e4hnlich, aber beide schlecht</li> </ul>"},{"location":"machine-learning/model_evaluation/#overfitting-vs-underfitting","title":"Overfitting vs. Underfitting","text":"Problem Training MSE Test MSE L\u00f6sung Perfekte Balance Niedrig Niedrig (\u00e4hnlich) \u2705 Ideal Overfitting Sehr niedrig Hoch Modell vereinfachen, mehr Daten Underfitting Hoch Hoch (\u00e4hnlich) Komplexeres Modell verwenden"},{"location":"machine-learning/model_evaluation/#implementierung-des-train-test-splits","title":"Implementierung des Train-Test Splits","text":"<pre><code># 80% der Daten f\u00fcr Training ausw\u00e4hlen\ntrain_data = data.sample(frac=0.8, random_state=42)\n# Restliche 20% f\u00fcr Test\ntest_data = data.drop(train_data.index)\n\n# Features und Target aufteilen\nX_train = train_data[['time']]\ny_train = train_data['temperature']\nX_test = test_data[['time']]\ny_test = test_data['temperature']\n</code></pre>"},{"location":"machine-learning/model_evaluation/#typische-aufteilungen","title":"Typische Aufteilungen","text":"Datensatzgr\u00f6sse Empfohlene Aufteilung Begr\u00fcndung &lt; 1000 Punkte 70-80% Training Mehr Daten f\u00fcr Training bei kleinen Datens\u00e4tzen 1000-10000 Punkte 80% Training, 20% Test Standard-Aufteilung &gt; 10000 Punkte 90% Training, 10% Test Bei grossen Datens\u00e4tzen reichen weniger Testdaten"},{"location":"machine-learning/model_evaluation/#modell-trainieren-und-evaluieren","title":"Modell trainieren und evaluieren","text":""},{"location":"machine-learning/model_evaluation/#1-modell-auf-trainingsdaten-trainieren","title":"1. Modell auf Trainingsdaten trainieren","text":"<pre><code>from sklearn.linear_model import LinearRegression\n\n# Modell erstellen und trainieren\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)  # NUR Trainingsdaten verwenden!\n</code></pre>"},{"location":"machine-learning/model_evaluation/#2-modell-auf-testdaten-evaluieren","title":"2. Modell auf Testdaten evaluieren","text":"<pre><code># Vorhersagen auf Testdaten machen\ny_pred = model.predict(X_test)\n\n# MSE manuell berechnen mit Residuen\nresiduals = y_test - y_pred\ntest_mse = (residuals ** 2).mean()\nprint(f\"Test MSE: {test_mse}\")\n</code></pre>"},{"location":"machine-learning/other_regression_models/","title":"Weitere Regressionen","text":""},{"location":"machine-learning/other_regression_models/#decision-tree-regressor","title":"Decision Tree Regressor","text":""},{"location":"machine-learning/other_regression_models/#was-ist-ein-decision-tree-regressor","title":"Was ist ein Decision Tree Regressor?","text":"<p>Der Decision Tree Regressor ist ein Modell, das Entscheidungen durch eine Baumstruktur trifft. Anstatt eine gerade Linie wie bei der linearen Regression zu finden, teilt der Decision Tree die Daten in verschiedene Bereiche auf und macht f\u00fcr jeden Bereich eine separate Vorhersage.</p> <p></p> <p>https://medium.com/berk-hakbilen/decision-trees-in-mexplained-a092e387e4aa</p>"},{"location":"machine-learning/other_regression_models/#wie-funktioniert-ein-decision-tree-regressor","title":"Wie funktioniert ein Decision Tree Regressor?","text":"<p>Der Algorithmus erstellt einen Baum von Entscheidungen, indem er die Daten schrittweise in kleinere Gruppen aufteilt. Jeder Knoten im Baum stellt eine Entscheidung dar (z.B. \"Ist x &gt; 5?\"), und die Bl\u00e4tter enthalten die finalen Vorhersagewerte.</p>"},{"location":"machine-learning/other_regression_models/#implementierung-des-decision-tree-regressors","title":"Implementierung des Decision Tree Regressors","text":"<pre><code>from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(max_depth=5)  # Begrenzt die Tiefe des Baums\nX = data[['x']]\ny = data['y']\nmodel.fit(X, y)\n\n# Vorhersage\npredictions = model.predict(X)\n</code></pre>"},{"location":"machine-learning/other_regression_models/#wichtiger-parameter-max_depth","title":"Wichtiger Parameter: max_depth","text":"<p>Der Parameter <code>max_depth</code> begrenzt die maximale Tiefe des Baums:</p> <ul> <li>Niedrige Werte (z.B. 2-5): Einfacheres Modell, weniger Overfitting</li> <li>Hohe Werte (z.B. 10+): Komplexeres Modell, kann zu Overfitting f\u00fchren</li> <li>None: Unbegrenzte Tiefe (Vorsicht vor Overfitting!)</li> </ul>"},{"location":"machine-learning/other_regression_models/#k-nearest-neighbors-regressor","title":"K-Nearest Neighbors Regressor","text":""},{"location":"machine-learning/other_regression_models/#was-ist-ein-k-nearest-neighbors-regressor","title":"Was ist ein K-Nearest Neighbors Regressor?","text":"<p>Der K-Nearest Neighbors (KNN) Regressor macht Vorhersagen basierend auf den k n\u00e4chstgelegenen Datenpunkten. F\u00fcr einen neuen Punkt betrachtet das Modell die k \u00e4hnlichsten Punkte aus den Trainingsdaten und berechnet deren Durchschnitt.</p> <p></p> <p>https://datasciencebook.ca/regression1.html#overview-6</p>"},{"location":"machine-learning/other_regression_models/#wie-funktioniert-ein-knn-regressor","title":"Wie funktioniert ein KNN Regressor?","text":"<ol> <li>F\u00fcr einen neuen Datenpunkt werden die k n\u00e4chstgelegenen Nachbarn in den Trainingsdaten gefunden</li> <li>Der Vorhersagewert wird als Durchschnitt der y-Werte dieser k Nachbarn berechnet</li> <li>Je n\u00e4her ein Nachbar ist, desto mehr Einfluss hat er auf die Vorhersage</li> </ol>"},{"location":"machine-learning/other_regression_models/#implementierung-des-knn-regressors","title":"Implementierung des KNN Regressors","text":"<pre><code>from sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor(n_neighbors=5)  # Verwendet 5 n\u00e4chste Nachbarn\nX = data[['x']]\ny = data['y']\nmodel.fit(X, y)\n\n# Vorhersage\npredictions = model.predict(X)\n</code></pre>"},{"location":"machine-learning/other_regression_models/#wichtiger-parameter-n_neighbors","title":"Wichtiger Parameter: n_neighbors","text":"<p>Der Parameter <code>n_neighbors</code> bestimmt, wie viele Nachbarn ber\u00fccksichtigt werden:</p> <ul> <li>Niedrige Werte (z.B. 1-3): Sehr flexibles Modell, kann zu Overfitting f\u00fchren</li> <li>Hohe Werte (z.B. 10+): Glatteres Modell, weniger sensitiv f\u00fcr einzelne Datenpunkte</li> </ul>"},{"location":"machine-learning/residual_analysis/","title":"Residuenanalyse: \u00dcberpr\u00fcfung der Modellannahmen mittels Residuenplot","text":"<p>Die Residuenanalyse hilft sicherzustellen, dass die Annahmen eines linearen Regressionsmodells korrekt sind. Ein Residuenplot ist ein einfaches und effektives Werkzeug, um diese Annahmen visuell zu \u00fcberpr\u00fcfen.</p>"},{"location":"machine-learning/residual_analysis/#wichtige-annahmen-der-residuen","title":"Wichtige Annahmen der Residuen","text":"<p>In einer linearen Regression gibt es drei grundlegende Annahmen bez\u00fcglich der Residuen:</p>"},{"location":"machine-learning/residual_analysis/#normalverteilung-der-residuen","title":"Normalverteilung der Residuen","text":"<p>Die Residuen sollten normalverteilt sein, damit die statistischen Tests und Sch\u00e4tzungen zuverl\u00e4ssig sind.</p> <p>\u00dcberpr\u00fcfung: Ein Residuenplot kann zeigen, ob die Residuen zuf\u00e4llig verteilt sind. Wenn die Residuen zuf\u00e4llig um die Null-Achse schwanken und keine Muster aufweisen, sind sie vermutlich normalverteilt.</p>"},{"location":"machine-learning/residual_analysis/#erwartungswert-von-0","title":"Erwartungswert von 0","text":"<p>Der Mittelwert der Residuen sollte null sein, was darauf hinweist, dass das Modell im Durchschnitt keine systematischen Fehler macht.</p> <p>\u00dcberpr\u00fcfung: Im Residuenplot sollte die Verteilung der Residuen um die Null-Achse zuf\u00e4llig verteilt sein. Wenn die Residuen systematisch \u00fcber oder unter der Null-Achse liegen, k\u00f6nnte der Mittelwert der Residuen von Null abweichen.</p>"},{"location":"machine-learning/residual_analysis/#unabhangigkeit-der-residuen","title":"Unabh\u00e4ngigkeit der Residuen","text":"<p>Die Residuen sollten unabh\u00e4ngig voneinander sein, d. h. der Fehler eines Datenpunkts sollte nicht vom Fehler eines anderen abh\u00e4ngen.</p> <p>\u00dcberpr\u00fcfung: Wenn die Residuen im Residuenplot ein Muster (z. B. eine Kurve) zeigen, k\u00f6nnte dies auf eine Abh\u00e4ngigkeit zwischen den Fehlern hinweisen.</p>"},{"location":"machine-learning/residual_analysis/#beispiel-eines-residuenplots","title":"Beispiel eines Residuenplots","text":"<p>Hier ist ein einfaches Beispiel, ein Residuenplot zu erstellen, um die oben genannten Annahmen zu \u00fcberpr\u00fcfen.</p> <pre><code>data['residuals'] = data['y'] - model.predict(data[['x']])\nplt.scatter(data['x'], data['residuals'])\nplt.axhline(0, color='red', linestyle='--')\nplt.title('Residuenplot')\nplt.xlabel('x')\nplt.ylabel('Residuen')\nplt.show()\n</code></pre> <p></p> <p>In diesem Beispiel sehen wir, dass die meisten Residuen um die Null-Achse verteilt sind und keine klaren Muster aufweisen. Dies deutet darauf hin, dass die Annahmen der Normalverteilung, des Mittelwerts von 0 und der Unabh\u00e4ngigkeit der Residuen erf\u00fcllt sind.</p>"},{"location":"statistics/correlation/","title":"Korrelation","text":""},{"location":"statistics/correlation/#korrelationen","title":"Korrelationen","text":"<p>Korrelationen messen den Zusammenhang zwischen zwei numerischen Variablen und dr\u00fccken ihn als Korrelationskoeffizient \\(R\\) aus. Wenn Sie ein intuitives Gef\u00fchl daf\u00fcr entwickeln m\u00f6chten, wie Korrelationskoeffizienten mit den Daten zusammenh\u00e4ngen, dann spielen Sie Guess the Correlation (gratis). </p>"},{"location":"statistics/correlation/#1-korrelationskoeffizient-r","title":"1. Korrelationskoeffizient \\(R\\)","text":"<ul> <li>Wertebereich: \\(-1 \\le R \\le 1\\)</li> <li>\\(R &gt; 0\\): positiver Zusammenhang (steigt X, steigt Y)</li> <li>\\(R &lt; 0\\): negativer Zusammenhang (steigt X, f\u00e4llt Y)</li> <li>\\(R == 0\\): kein Zusammenhang (X und Y sind unabh\u00e4ngig voneinander)</li> <li>\\(|R| &gt; 0.8\\): starke Korrelation</li> <li>\\(|R|\\) zwischen \\(0.5\\) und \\(0.8\\): moderate Korrelation</li> <li>\\(|R| &lt; 0.5\\): schwache Korrelation</li> </ul> <p>Wichtig: Korrelation \u2260 Kausalit\u00e4t! Ein hoher Korrelationswert zeigt nur, dass zwei Variablen simultan variieren, aber nicht, dass die eine die Ursache der anderen ist oder umgekehrt. Wie die Korrelation zu interpretieren ist, findet man am besten heraus, indem man sich die Frage stellt: Wieso w\u00fcrde die Messreihe X eine Wirkung in der Messreihe Y oder umgekehrt ergeben? Gibt es eine weitere dritte Variable die die Ursache f\u00fcr X und Y sein k\u00f6nnte?</p> <p>In der unterstehenden Grafik sind verschiedene Korrelationskoeffizienten (Pearson) und die dazugeh\u00f6rigen Daten zu sehen. Es ist gut ersichtlich, dass die Steigung der Daten keinen Einfluss auf die Korrelationskoeffizienten hat (Reihe 2).</p> <p></p>"},{"location":"statistics/correlation/#2-pearson-vs-spearman","title":"2. Pearson vs. Spearman","text":"<p>Es gibt viele Arten von Korrelationen. Haupts\u00e4chlich werden aber zwei Methoden angewendet: die Pearson-Korrelation und die Spearman-Korrelation.</p> <p>Die Pearson-Korrelation ist simple und kann verwendet werden, um lineare Zusammenh\u00e4nge zu messen. </p> <p>Was aber, wenn ein Zusammenhang nicht linear ist, zum Beispiel quadrilinear, quadratisch oder exponentiell? In diesem Fall kann die Pearson-Korrelation dies nicht mehr messen, und hier verwendet man die Spearman-Korrelation. </p> <p>Ausserdem ist die Spearman-Korrelation fast nicht anf\u00e4llig gegen\u00fcber Ausreissern. </p> Merkmal Pearson Spearman Zusammenhangstyp Linear Monoton (beliebig nicht-linear) Sensitivit\u00e4t Ausreisser Hoch Gering (robust gegen\u00fcber Ausreissern) <ul> <li>Pearson misst die lineare Abh\u00e4ngigkeit.</li> <li>Spearman misst, ob eine monotone Beziehung vorliegt, und ist robuster gegen\u00fcber Ausreissern und nicht-linearen Mustern.</li> </ul> <p>In der nachfolgenden Grafik ist noch einmal klar dargestellt, was mit einer linearen Abh\u00e4ngigkeit und einer monotonen Beziehung gemeint ist. </p> <p>Hier sind die Daten nicht linear abh\u00e4ngig, haben jedoch eine monotone Beziehung (Y erh\u00f6ht sich IMMER wenn sich X erh\u00f6ht und umgekehrt). </p> <p></p>"},{"location":"statistics/correlation/#3-beispielcode-pseudodaten","title":"3. Beispielcode (Pseudodaten)","text":"<pre><code>import numpy as np\nimport pandas as pd\n\n# Pseudodaten erzeugen\nnp.random.seed(42)\nx = np.random.normal(loc=50, scale=10, size=100)\ny_linear = 0.8 * x + np.random.normal(scale=5, size=100)    # \u00fcberwiegend linear\ny_nonlinear = x**4 # monotone, nicht-linear\ny_ausreisser = x.copy()\ny_ausreisser[-1] = -1000 # Ausreisser\n\n\ndf = pd.DataFrame({'Temperatur': x, 'Licht': y_linear, 'Bewegungssensor': y_nonlinear, 'Ausreisser': y_ausreisser})\n\n# Pearson-Korrelation\nprint(\"Pearson R:\")\n\n# Pandas berechnet standardm\u00e4ssig die Pearson-Korrelation\nprint(df.corr())\n\n# Spearman-Korrelation\nprint(\"Spearman R:\")\nprint(df.corr(method=\"spearman\"))\n\n# Die Korrelationsmatrix l\u00e4sst sich auch visualisieren\n# Pearson-Korrelation\npearson_corr = df.corr().round(2)\npearson_text = pearson_corr.astype(str)\n\nfig_pearson = go.Figure(data=go.Heatmap(\n    z=pearson_corr.values,\n    x=pearson_corr.columns,\n    y=pearson_corr.columns,\n    text=pearson_text.values,\n    texttemplate=\"%{text}\",\n    textfont={\"size\":14, \"color\":\"black\"},\n    colorscale='RdBu',\n    zmin=-1,\n    zmax=1,\n    colorbar=dict(title=\"Pearson R\")\n))\n\nfig_pearson.update_layout(\n    title=\"Korrelationsmatrix (Pearson)\",\n    xaxis_title=\"Variablen\",\n    yaxis_title=\"Variablen\"\n)\n\nfig_pearson.show()\n</code></pre> <p>Ausgabe: <pre><code>Pearson R:\n                 Temperatur     Licht  Bewegungssensor  Ausreisser\nTemperatur         1.000000  0.813799         0.944682    0.100723\nLicht              0.813799  1.000000         0.810350    0.155170\nBewegungssensor    0.944682  0.810350         1.000000    0.118911\nAusreisser         0.100723  0.155170         0.118911    1.000000\n\nSpearman R:\n                 Temperatur     Licht  Bewegungssensor  Ausreisser\nTemperatur         1.000000  0.787855         1.000000    0.988647\nLicht              0.787855  1.000000         0.787855    0.792439\nBewegungssensor    1.000000  0.787855         1.000000    0.988647\nAusreisser         0.988647  0.792439         0.988647    1.000000\n</code></pre></p> <p></p> <p>Erwartete Ergebnisse:</p> <ul> <li>Pearson R (linear: Temperatur~Licht) \u2248 0.81 \u2192 starke lineare Korrelation</li> <li>Spearman R (nicht-linear: Temperatur~Bewegungssensor) \u2248 1 \u2192 perfekte Korrelation statt nur 0.94</li> <li>Spearman R (ausreisser: Temperatur~Ausreisser) \u2248 0.989 \u2192 Der Ausreisser hat keinen Einfluss auf den Korrelationskoeffizienten. Pearson R ist nur 0.1.</li> </ul> <p>Diese Ergebnisse lassen sich auch noch visuell \u00fcberpor\u00fcfen:</p> <pre><code>fig = make_subplots(rows=1, cols=3, subplot_titles=('Temperatur vs. Licht', 'Temperatur vs. Bewegungssensor', 'Temperatur vs. Ausreisser'))\n\nfig.add_trace(go.Scatter(x=df['Temperatur'], y=df['Licht'], mode='markers', name='Licht'), row=1, col=1)\nfig.update_xaxes(title_text='Temperatur', row=1, col=1)\nfig.update_yaxes(title_text='Licht', row=1, col=1)\n\nfig.add_trace(go.Scatter(x=df['Temperatur'], y=df['Bewegungssensor'], mode='markers', name='Bewegungssensor'), row=1, col=2)\nfig.update_xaxes(title_text='Temperatur', row=1, col=2)\nfig.update_yaxes(title_text='Bewegungssensor', row=1, col=2)\n\nfig.add_trace(go.Scatter(x=df['Temperatur'], y=df['Ausreisser'], mode='markers', name='Ausreisser'), row=1, col=3)\nfig.update_xaxes(title_text='Temperatur', row=1, col=3)\nfig.update_yaxes(title_text='Ausreisser', row=1, col=3)\n\nfig.update_layout(title_text='Scatterplots von Temperatur vs. Anderen Merkmalen')\nfig.show()\n</code></pre> <p></p>"},{"location":"statistics/correlation/#4-interpretation","title":"4. Interpretation","text":"<ul> <li>R nahe \u00b11: sehr starker Zusammenhang</li> <li>R nahe 0: kein oder sehr schwacher Zusammenhang</li> </ul>"},{"location":"statistics/univariate_statistics/","title":"Univariate Statistiken","text":"<p>Die Funktion <code>df.describe()</code> in Pandas berechnet automatisch eine Reihe von statistischen Kennzahlen f\u00fcr numerische Spalten in einem DataFrame. Diese Kennzahlen geben Ihnen einen ersten \u00dcberblick \u00fcber die Verteilung Ihrer Daten.</p>"},{"location":"statistics/univariate_statistics/#welche-kennzahlen-liefert-describe","title":"Welche Kennzahlen liefert <code>describe()</code>?","text":"<p>Standardm\u00e4ssig berechnet <code>describe()</code> acht statistische Masse f\u00fcr jede numerische Spalte:</p> <ol> <li>count: Anzahl der nicht-null Werte</li> <li>mean: Arithmetisches Mittel (Durchschnitt)</li> <li>std: Standardabweichung (Streuungsmass)</li> <li>min: Minimaler Wert</li> <li>25%: 1. Quartil (25. Perzentil)</li> <li>50%: Median (2. Quartil, 50. Perzentil)</li> <li>75%: 3. Quartil (75. Perzentil)</li> <li>max: Maximaler Wert</li> </ol>"},{"location":"statistics/univariate_statistics/#count-anzahl","title":"Count (Anzahl)","text":"<p>Die Anzahl der g\u00fcltigen (nicht-null) Werte in der Spalte. Wichtig zur Erkennung fehlender Daten und zur Einsch\u00e4tzung der Datenbasis.</p>"},{"location":"statistics/univariate_statistics/#mean-mittelwert","title":"Mean (Mittelwert)","text":"<p>Das arithmetische Mittel - die Summe aller Werte geteilt durch ihre Anzahl. Sehr empfindlich gegen\u00fcber extremen Werten (Ausreissern). Bei stark asymmetrischen Verteilungen kann der Mittelwert irref\u00fchrend sein.</p>"},{"location":"statistics/univariate_statistics/#min-und-max","title":"Min und Max","text":"<p>Die kleinsten und gr\u00f6ssten Werte in den Daten. Zusammen definieren sie die Spannweite (Range). Sehr empfindlich f\u00fcr Ausreisser und Dateneingabefehler.</p>"},{"location":"statistics/univariate_statistics/#quartile-und-median","title":"Quartile und Median","text":"<p>Die Quartile teilen die Daten in vier gleich grosse Teile, nachdem sie der Gr\u00f6sse nach sortiert wurden:</p> <ul> <li>25% (1. Quartil/Q1): Ein Viertel der Daten hat Werte kleiner oder gleich diesem Wert</li> <li>50% (2. Quartil/Q2 = Median): Die H\u00e4lfte der Daten liegt unter diesem Wert, die andere H\u00e4lfte dar\u00fcber</li> <li>75% (3. Quartil/Q3): Drei Viertel der Daten haben Werte kleiner oder gleich diesem Wert</li> </ul>"},{"location":"statistics/univariate_statistics/#die-normalverteilung-und-die-standardabweichung","title":"Die Normalverteilung und die Standardabweichung","text":"<p>Die Normalverteilung ist eine glockenf\u00f6rmige Kurve, in der sich die meisten Werte in der Mitte (dem Durchschnitt) sammeln und die H\u00e4ufigkeit nach aussen hin abnimmt. Die Kurve ist symmetrisch - links und rechts vom Durchschnitt ist das Muster gleich.</p> <p></p> <p>Die Standardabweichung bestimmt dabei die \"Breite\" dieser Glocke. Sie gibt an, wie stark die Werte um den Durchschnitt streuen:</p> <ul> <li>Eine kleine Standardabweichung erzeugt eine schmale, hohe Glocke: Die Werte liegen eng beieinander</li> <li>Eine grosse Standardabweichung erzeugt eine breite, flache Glocke: Die Werte sind weit verstreut</li> </ul> <p>Die wichtigste Eigenschaft dieser Kombination: In einer Normalverteilung liegen:</p> <ul> <li>68% aller Werte innerhalb von 1 Standardabweichung um den Mittelwert</li> <li>95% innerhalb von 2 Standardabweichungen</li> <li>99,7% innerhalb von 3 Standardabweichungen</li> </ul>"},{"location":"statistics/visualization/","title":"Visualisierungen","text":""},{"location":"statistics/visualization/#visualisierungen","title":"Visualisierungen","text":"<p>Daten-Visualisierungen helfen, Muster, Ausreisser und Zusammenh\u00e4nge in Daten schnell zu erfassen. Im Folgenden vier grundlegende Diagrammtypen:</p> <ol> <li>Line Plot (Linienplot)</li> <li>Histogramm</li> <li>Bar Plot (Balkendiagramm)</li> <li>Scatter Plot (Punktwolke)</li> </ol>"},{"location":"statistics/visualization/#1-line-plot-linienplot","title":"1. Line Plot (Linienplot)","text":"<p>Ein Linienplot eignet sich besonders f\u00fcr Zeitreihen oder geordnete Daten, bei denen die x-Achse eine nat\u00fcrliche Reihenfolge (z. B. Zeitstempel) hat. Die Verbindung durch Linien betont Trends und Verl\u00e4ufe.</p> <p>Anwendungsf\u00e4lle</p> <ul> <li>Sensor- oder Messwerte \u00fcber die Zeit</li> <li>Temperaturverlauf, Umsatzentwicklung</li> </ul> <p>Beispielcode</p> <pre><code>import pandas as pd\nimport plotly.graph_objects as go\n\n# Pseudodaten: t\u00e4gliche Temperaturwerte\ndates = pd.date_range('2025-01-01', periods=10, freq='D')\ntemperature = [5, 7, 6, 8, 10, 9, 11, 10.5, 12, 13]\ndf = pd.DataFrame({'Datum': dates, 'Temperatur': temperature})\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df['Datum'], y=df['Temperatur'], mode='markers+lines'))\n\nfig.update_layout(\n    title='T\u00e4gliche Temperaturwerte',\n    xaxis_title='Datum',\n    yaxis_title='Temperatur'\n)\n\nfig.show()\n</code></pre> <p></p>"},{"location":"statistics/visualization/#zwei-linien","title":"Zwei Linien","text":"<p>Um zwei Linien in der gleichen Grafik darzustellen, kann ein weiterer Trace hinzugef\u00fcgt werden. </p> <p>Wichtig ist, dass jetzt auch die Eigenschaft Name gesetzt wird.</p> <p>Beispielcode</p> <pre><code>import pandas as pd\nimport plotly.graph_objects as go\n\n# Pseudodaten: t\u00e4gliche Temperaturwerte f\u00fcr zwei Orte\ndates = pd.date_range('2025-01-01', periods=10, freq='D')\ntemperature_room1 = [5, 7, 6, 8, 10, 9, 11, 10.5, 12, 13]\ntemperature_room2 = [3, 4, 5, 6.5, 7, 8, 8.5, 9, 10, 10.5]\n\ndf = pd.DataFrame({\n    'Datum': dates,\n    'Raum A': temperature_room1,\n    'Raum B': temperature_room2\n})\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df['Datum'], y=df['Raum A'], mode='markers+lines', name='Raum A'))\nfig.add_trace(go.Scatter(x=df['Datum'], y=df['Raum B'], mode='markers+lines', name='Raum B'))\n\nfig.update_layout(\n    title='T\u00e4gliche Temperaturwerte in zwei St\u00e4dten',\n    xaxis_title='Datum',\n    yaxis_title='Temperatur (\u00b0C)',\n    legend_title='Raum'\n)\n\nfig.show()\n</code></pre> <p></p>"},{"location":"statistics/visualization/#2-histogramm","title":"2. Histogramm","text":"<p>Ein Histogramm zeigt die Verteilung einer numerischen Variable, indem es den Wertebereich in Intervalle (\u201eBins\u201c) unterteilt und die H\u00e4ufigkeit der Werte in jedem Bin z\u00e4hlt.</p> <p>Anwendungsf\u00e4lle</p> <ul> <li>Verteilungen von Messwerten (z. B. CO\u2082-Konzentrationen, Testscores)</li> <li>Identifikation von Schiefe der Verteilung oder Ausreissern</li> </ul> <p>Beispielcode</p> <p><pre><code>import numpy as np\nimport plotly.graph_objects as go\n\ndata = np.random.normal(loc=50, scale=10, size=500)\n\nfig = go.Figure(data=[go.Histogram(x=data, nbinsx=15)])\n\nfig.update_layout(\n    title='Histogramm der Luftfeuchtigkeiten',\n    xaxis_title='Luftfeuchtigkeit',\n    yaxis_title='H\u00e4ufigkeit'\n)\n\nfig.show()\n</code></pre> </p>"},{"location":"statistics/visualization/#3-bar-plot-balkendiagramm","title":"3. Bar Plot (Balkendiagramm)","text":"<p>Ein Bar Plot vergleicht kategoriale Werte oder aggregierte Gr\u00f6dden. Die x-Achse zeigt Kategorien, die y-Achse eine numerische Metrik (z. B. Mittelwerte, Summen).</p> <p>Anwendungsf\u00e4lle</p> <ul> <li>Durchschnittswerte pro Gruppe (z. B. Durchschnitts-CO\u2082 pro Raum)</li> <li>Summen, Z\u00e4hlungen je Kategorie (z. B. Anzahl Messwerte pro Raum)</li> </ul> <p>Beispielcode</p> <p><pre><code>import pandas as pd\nimport plotly.graph_objects as go\n\n# Beispiel f\u00fcr Temperaturdaten pro Raum\ndata = {'Raum': ['Raum 1', 'Raum 2', 'Raum 1', 'Raum 2', 'Raum 3'],\n        'Temperatur': [20, 22, 21, 23, 24]}\ntemp_df = pd.DataFrame(data)\n\n# Berechnung der durchschnittlichen Temperatur pro Raum\naverage_temp = temp_df.groupby('Raum')['Temperatur'].mean().reset_index()\n\nfig = go.Figure(data=[go.Bar(x=average_temp[\"Raum\"], y=average_temp[\"Temperatur\"])])\n\nfig.update_layout(\n    title='Temperatur pro Raum',\n    xaxis_title='Raum',\n    yaxis_title='Temperatur'\n)\n\nfig.show()\n</code></pre> </p>"},{"location":"statistics/visualization/#4-scatter-plot-punktwolke","title":"4. Scatter Plot (Punktwolke)","text":"<p>Ein Scatter Plot zeigt die Beziehung zwischen zwei numerischen Variablen als Punktwolke. Er hilft, Zusammenh\u00e4nge, Cluster und Ausreisser zu erkennen.</p> <p>Anwendungsf\u00e4lle</p> <ul> <li>Korrelation zwischen zwei Merkmalen</li> <li>Zusammenhang mit dem Auge absch\u00e4tzen</li> </ul> <p>Beispielcode</p> <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Pseudodaten: zwei numerische Merkmale\nnp.random.seed(0)\nx = np.random.uniform(0, 10, size=100) + 16    # z. B. Temperatur\ny = 2.5 * x + np.random.normal(0, 5, size=100) # z. B. Luftfeuchtigkeit\n\ndf = pd.DataFrame({'Temperatur': x, 'Feuchtigkeit': y})\n\nfig = go.Figure(data=[go.Scatter(x=df['Temperatur'], y=df['Feuchtigkeit'], mode='markers')])\n\nfig.update_layout(\n    title='Temperatur vs. Luftfeuchtigkeit',\n    xaxis_title='Temperatur (\u00b0C)',\n    yaxis_title='Luftfeuchtigkeit (%)'\n)\n\nfig.show()\n</code></pre> <p></p>"},{"location":"statistics/visualization/#zusammenfassung","title":"Zusammenfassung","text":"Diagramm Zweck Line Plot Trends und Verl\u00e4ufe \u00fcber geordnete Daten Histogramm Verteilung einer einzelnen Variable Bar Plot Vergleich zwischen Gruppen Scatter Plot Beziehung/Korrelation zwischen Merkmalen"}]}